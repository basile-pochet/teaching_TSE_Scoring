---
title: "Logistic regression"
author: "Louis OLIVE"
bibliography: references.bib
link-citations: true
format:
  html:
    #theme: darkly
    highlight: espresso
    code-copy: true
    code-fold: true
    df-print: paged
    #include-in-header: mathjax.html
    number-sections: true
    toc: true
    toc_depth: 3
    toc_float: yes
execute: 
  cache: true
  warning: false
editor: visual
fontsize: 11pt
---

```{r, echo = FALSE, warning = FALSE, message = FALSE}
library(tidyverse)
```

# The Logistic Regression model

## Informal introductory example

We provide here some intuitions leading to the Logistic Regression model using a simulated data set from @islr2021 (the **Default** data set).

This is a toy data set used for teaching purposes containing information on ten thousand customers.

The aim here is to assess which customers will ***default*** on their credit card debt (the target or response variable) based on the current credit card ***balance*** and other individual characteristics (the predictors or feature vector).

This is a binary classification problem as the ***default*** variable takes value in a discrete set (here binary). In the following we will denote $Y$ the output or response variable and $X = (X_0,X_1,\cdots,X_{p-1})^{T}$ the feature vector or inputs.

The approach we follow is similar to [@hosmer2013] or [@cornillon2019]. Both books use a similar example: the presence or absence of a Coronary Heart Disease (CHD) is explained with the age of an individual (the data set *chdage* is available in companion package *aplore3*).

We can start to explore the Default data with a scatterplot (@fig-default_balance-scatterplot) of the target variable (***default***) with respect to a predictor (***balance***):

```{r}
#| label: fig-default_balance-scatterplot
#| fig-cap: "Scatterplot of variable ***default*** with respect to credit card ***balance*** for 10000 customers"

# Default data set (simulated) from ESLII/ISLR
default_data <- ISLR2::Default %>%
    as_tibble()

ggplot(default_data, aes(x=balance, y=default)) +
geom_point(alpha=0.2)
```

In this scatterplot, all points fall on one of two parallel lines representing the absence (No) or occurrence (Yes) of ***default***. We "jitter" the data vertically to avoid overplotting. The plot below shows that the response variable is imbalanced towards the absence of default:

```{r}
ggplot(default_data, aes(x=balance, y=default)) +
geom_jitter(alpha=0.2, height=0.2)
```

We also show the boxplots of credit cards ***balance*** with respect to ***default*** status:

```{r}
#| label: fig-balance_default-boxplot
#| fig-cap: "Variable ***balance*** with respect to ***default*** status"
ggplot(default_data,
       aes(x=default, y=balance)) +
geom_boxplot()
```

We can see from @fig-default_balance-scatterplot and @fig-balance_default-boxplot that default tends to be more prevalent for accounts with a high balance. However it is difficult to guess a simple relationship between default and balance.

To investigate further we discretise the balance variables by classes of width $300\$$ and compute the mean of response variable (***default*** is Yes) within each balance class:

```{r, message = FALSE}
class_width <- 300
(default_data_binned <- default_data %>%
    mutate(balance_bins = cut(balance, breaks = seq(0, 3000, class_width),
                              right = FALSE, dig.lab = 4),
           min = floor(balance / class_width) * class_width,
           max = if_else(balance == 0 , 1, 
                         # customers with 0$ balance should be long to [0, width) class
                         # or be excluded
                         ceiling(balance / class_width))  * class_width) %>% 
    group_by(balance_bins, min, max, default) %>% 
    summarize(n=n()) %>% 
    ungroup() %>% 
    pivot_wider(names_from = default, values_from = n) %>%
    replace_na(list(Yes = 0, No = 0)) %>% 
    mutate(`Mean(default)` = round(Yes / (Yes + No), 4)))
```

In the following we map, by convention and for better readability, the response variable $Y \in\{Yes, No\}$ to $\{0, 1\}$:

$$
Y = \left\{ \begin{array}{ll} 
    1&  \mbox{if customer defaulted on its credit card (ie default=Yes)}\cr
    0&  \mbox{otherwise}.
    \end{array} \right.
$$

Then we plot the mean of default (in red) within each balance class (of width $300\$$):

```{r}
#| label: fig-balance_default-scatterplot-occurrence
#| fig-cap: "Mean occurrence of ***default*** within ***balance*** classes"

(default_occurrence <- 
 ggplot(default_data %>% mutate(default = if_else(default == "Yes", 1, 0)), aes(x=balance, y=default)) +
 geom_point(alpha=0.2) +
 geom_segment(data = default_data_binned,
              aes(x = min, xend = max, y = `Mean(default)`, yend = `Mean(default)`),
              color = 'coral',
              linewidth=1.25))
```

The relationship between the mean occurrence of ***default*** and ***balance*** is easier to read.

@fig-balance_default-scatterplot-occurrence clearly shows that as balance increases, the proportion of customers defaulting on their credit card increases.

We also notice that the mean default occurrence with respect to balance classes follows a kind of "S"-shaped curve or **sigmoid** function. Note that the shape depends on classes width and might change.

Going further and informally, considering that the mean of default occurrence is an estimate of $\mathbf{E}[Y|X=x]$ for each balance classes an idea would be to model:

$$
 \mathbb{E}[Y|X=x] = \mu_\beta(x)
$$

where $\mu_\beta$ is a **sigmoid** function in $[0,1]$.

The Logistic Regression model uses the **sigmoid** function $\sigma: x \to\sigma(x)=\frac{e^{x}} { 1 + e^{x} }$ also known as logistic function.

```{r}
(default_occurrence +
 geom_smooth(method = "glm", 
             formula = y ~ x,
             method.args = list(family = "binomial"), 
             se = FALSE,
             col = "dodgerblue",
             linetype = "dotted") +
 geom_segment(data = default_data_binned,
              aes(x = min, xend = max, y = `Mean(default)`, yend = `Mean(default)`),
              color = 'coral',
              linewidth=1.25))
```

Another approach would have been to treat $Y$ as a quantitative response variable and fit a simple linear model:

```{r}
linear <- lm(default ~ balance, data = default_data %>% mutate(default = if_else(default == "Yes", 1, 0)))
#summary(linear)
coeff_lm <- linear$coefficients
alpha <- coeff_lm["(Intercept)"]
beta <- coeff_lm["balance"]
```

```{r, warning = FALSE}
#| label: fig-balance_default-scatterplot-lmfit
#| fig-cap: "Fitting a linear model, ***default*** is treated as a quantitative response variable"

(ggplot(default_data %>% mutate(default = if_else(default == "Yes", 1, 0)), aes(x=balance, y=default)) +
 geom_point(alpha=0.2) +
 geom_segment(data = default_data_binned,
              aes(x = min, xend = max, y = `Mean(default)`, yend = `Mean(default)`),
              color = 'coral',
              size=1.25)+
 geom_line(data = tibble(x = seq(0,3000, 300)) %>%
               mutate(y = alpha + beta * x),
           aes(x = x, y = y),
           color = 'darkolivegreen',
           linetype = 'dotted'))
 # Alternatively we could have used the geom_smooth command
 # geom_smooth(method = "lm", 
 #             formula = y ~ x,
 #             se = FALSE,
 #             col = "darkolivegreen",
 #             linetype = "dotted")

```

@fig-balance_default-scatterplot-lmfit shows that a linear model fails to fit the data. In particular, for low credit card balances the linear model shows a "negative probability of default". This is quite prominent here as response variable is imbalanced towards the No default category. For the same reasons, the least square method fails to correctly fit the category of interest (less than 0.25 probability).

Usually in such presentation (for example @hosmer2013) data is more balanced and a linear model approximately fits the two classes. However in any case a linear model cannot confine the predicted value to $[0, 1]$ for all observations of predictors.

## A more formal definition

We use the concepts we have defined in the first part of this lesson. The problem we are facing trying to predict the output default ($Y \in(0,1)$) using a training set of inputs or featur vector $X$ is a binary classification problem.

We remind that to estimate an optimal classifier for output $Y \in \{0,1\}$ using input $X = (X_1,\cdots,X_p)$ one approach was to:

-   model the joint distribution (or generative distribution) between $X$ and $Y$,

-   estimate $\eta(x)$ with:

$$
\eta(x)=\mathbb{P}[Y=1|X=x)] = \mathbb{E}[Y=1|X=x)]
$$

-   $\eta(x)$ can be used as a Scoring function (ROC curve, choice of cutoff $s$)

-   and we can use the classifier (usually $s=\frac{1}{2}$) to predict output $Y$:

$$
f_s(x)=\left\{ \begin{array}{ll} 
    1 &  \mbox{if } \eta(x)\geq s\cr
    0 &  \mbox{otherwise}\cr
\end{array} \right.
$$

In the case of **Logistic Regression** classifier, we model:

$$
Y|X=x~ \sim B(\eta(x))
$$

with

$$
\eta(x)=\sigma(x^T\beta) =\frac{exp(x^T\beta)}{1+exp(x^T\beta)}
$$

for some parameter $\beta=(\beta_1,\cdots,\beta_p)\in \mathbb R^p$, usually $x_1=1$ and $\beta_1$ is an intercept. $\sigma$ is the sigmoid logistic function we have seen before.

In the literature is usual to denote $\eta(X)=p_{\beta}(X)$ or $\eta(X)=\pi_{\beta}(X)$.

From now, we will use the notation $p_{\beta}(X)$.

Defining $\mathrm{logit}: x \to \log\bigg( \frac{x}{1-x}\bigg)$ we have:

$$
\mathrm{logit}(p_{\beta}(X))=X^T\beta
$$

::: {.callout-tip icon="false"}
## The **Logistic Regression** model:

We are given $(x_i, y_i) \in \mathbb R^p \times \{0,1\}$, $i=1,\cdots,n$

The Logistic Regression model assumes that outputs $y_i$ are independent Bernoulli with parameter $p_{\beta}(x_i)$ depending on $x_i$:

$$
\mathrm{logit}(p_{\beta}(x_i))=x_i^T\beta
$$
:::

The Logistic Regression model defined above is a special case of more general family of models, the so-called Generalized Linear Models (GLM).

The family of GLM extends the applicability of linear-model ideas to data where responses are binary (e.g. Logistic Regression) or counts (e.g. Poisson Regression), among further possibilities. The concept emerged with Nelder and Wedderburn and has been studied extensively (see @nelder1972 or @cornillon2019) .

The syntax to fit the Logistic model in R using `glm()` is:

$$
\mathtt{glm(} \mathrm{y} \sim \mathrm{~x,~}\mathtt{data=}~\mathrm{dataframe,~}\mathtt{family~=~binomial(link~=~"logit")}
$$

The formula $\mathrm{y} \sim \mathrm{x}$ depicts the model (i.e. inputs are $X$, output is $Y$) and the `data=` argument points to the training set contained in a R dataframe (or tibble). This is quite similar to the `lm()` function.

We also need to specify the distribution for the conditional $Y$ values (binomial) and the link function (logit) via the `family=` argument.

For our example:

```{r}
#| code-fold: show
glm_default <- glm(default ~ 1 + student + balance + income,
                   data = default_data,
                   family = binomial(link = "logit"))

glm_default <- glm(default ~ .,
                   data = default_data,
                   family = "binomial") # by default: link = "logit"
 
```

The command `summary` produces result summaries of the fitted model:

```{r}
#| code-fold: show
summary(glm_default)
```

We will see later how to interpret or understand what is printed

A coefficient-wise output of the model can be obtained as a `tibble` using `tidy()` from package `broom`:

```{r}
broom::tidy(glm_default)
```

Based on this output, the fitted model is (we have re-scaled balance and income for better readability):

$$
\log \bigg( \frac{p_{\beta}(x_i)}{1 - p_{\beta}(x_i)}\bigg) = -1.08 - 0.65(\mathbb{1}_{\mathrm{student}_i=\mathrm{Yes}}) + 5.74(\frac{\mathrm{balance}_i}{1000})+ 0.03(\frac{\mathrm{income}_i}{10000})
$$

## Estimation

### Maximum Likelihood Estimation

We are given $(x_i, y_i) \in \mathbb R^p \times \{0,1\}$, $i=1,\cdots,n$ where outputs $y_i$ are independent Bernoulli with parameter $p_{\beta}(x_i)$ depending on $x_i$:

$$
\mathrm{logit}(p_{\beta}(x_i))=x_i^T\beta
$$

The parameters $\beta$ of the Logistic Regression model are usually determined using Maximum Likelihood Estimation (MLE). It consists on finding $\beta$ for which the joint probability of the observed data is greatest.

As $y_i$ are independent the likelihood function (joint probability) is the product of the probability mass functions:

$$
L(Y,\beta) = \prod_{i=1}^n p_{\beta}(x_i)^{y_i}(1-p_{\beta}(x_i))^{1-y_i}
$$ 

with $Y=(y_1,\cdots,y_n)$ and $\beta=(\beta_1,\cdots,\beta_p)$.

We seek to maximize the likelihood function over $\beta$, it is equivalent but easier to maximize the log-likelihood:

$$
\begin{align}
\ell(Y,\beta)=\log L(Y,\beta) &=\sum_{i=1}^n \left(y_i \log(p_{\beta}(x_i))+(1-y_i) \log(1- p_{\beta}(x_i))\right) \\
                              &=\sum_{i=1}^{n} \left(y_i \log(\frac{p_{\beta}(x_i)}{1-p_{\beta}(x_i)})+\log(1- p_{\beta}(x_i))\right)\\ 
                              &=\sum_{i=1}^{n} \left(y_i x_i^T\beta -\log(1 + \exp(x_i^T\beta)\right)
\end{align} 
$$

If the MLE $\hat\beta$ exists, the gradient of log-likelihood satisfies (first order necessary condition):

$$
\nabla\ell(Y,\beta)=\left(\frac{\partial \ell(Y,\beta)}{\partial \beta_1}, \cdots,\frac{\partial \ell(Y,\beta)}{\partial \beta_p}\right)=\mathbf 0
$$

We have for $j=1,\cdots,p$:

$$
\frac{\partial \ell(Y,\beta)}{\partial \beta_j}=\sum_{i=1}^{n} \left(y_i x_{ij} -x_{ij}\frac{\exp(x_i^T\beta)}{1+\exp(x_i^T\beta)}\right)=\sum_{i=1}^{n} x_{ij} \left(y_i- p_{\beta}(x_i)\right)
$$

In vector form:

$$
\nabla\ell(Y,\beta)=\sum_{i=1}^{n} x_{i} \left(y_i- p_{\beta}(x_i)\right)=X^T(Y-P_{\beta})
$$

where:

$$
\begin{align}
X = \begin{pmatrix}
x_{11} & \cdots & x_{1p} \\
x_{21} & \cdots & x_{2p} \\
\vdots  & \vdots  & \vdots   \\
x_{n1} & \cdots & x_{np} 
\end{pmatrix} = 
\begin{pmatrix}
 x_1^T\\
 x_2^T\\
\vdots \\
 x_n^T
\end{pmatrix}\in \mathbb{R}^{n\times (p)}, \quad
Y = \begin{pmatrix}
y_{1} \\
y_{2}\\
\vdots \\
y_{n}  
\end{pmatrix} \quad and \quad
P_{\beta} = \begin{pmatrix}
p_{\beta}(x_1) \\
p_{\beta}(x_2)\\
\vdots \\
p_{\beta}(x_n)  
\end{pmatrix} 
\end{align}
$$

In the literature $\nabla\ell(Y,\beta)$ is denoted as the Fisher's score function $S(\beta)$, if the MLE $\hat\beta$ exists, we have:

$$
S(\hat\beta)=\nabla\ell(Y,\hat\beta)=X^T(Y-P_{\hat\beta})=0
$$

Solving this equation involves solving $p$ non-linear equations in $\beta$:

$$
y_1 x_{1j} + \cdots + y_n x_{nj} = x_{1j}\frac{\exp(x_1^T\beta)}{1+\exp(x_1^T\beta)}+ \cdots + x_{nj}\frac{\exp(x_n^T\beta)}{1+\exp(x_n^T\beta)},\quad j=1,\cdots,p
$$

### Numerical methods

In practice we use numerical methods to solve these non-linear equations as no closed-form solution exist.

If we assume that $rank(X)=p$, we will have that $S(\beta)$ is concave in $\beta$ hence if we find a local maximum it is a global maximum.

We have for $(k,l) \in (1,\cdots,p)^2$:

$$
\begin{align}
\frac{\partial\mathcal \ell}{\partial\beta_k\partial\beta_l}(\beta)= & \frac{\partial}{\partial\beta_k}
\sum_{i=1}^nx_{il}(y_i-\frac{\exp(x_i^T\beta)}{1+\exp(x_i^T\beta)}) \\
=& -\sum_{i=1}^nx_{il}x_{ik}\frac{\exp(x_i^T\beta)}{(1+\exp(x_i^T\beta))^2} \\
=& -\sum_{i=1}^nx_{ik}p_\beta(x_i)(1-p_\beta(x_i))x_{il}
\end{align}
$$

We obtain that in matrix form:

$$
H(\beta)=\nabla^2\ell(Y,\beta)=-X^T W_\beta X
$$

where:

$$ 
\begin{align}
W_\beta = \begin{pmatrix}
p_\beta(x_1)(1-p_\beta(x_1)) & \cdots & \cdots\\
\vdots  & \ddots & \vdots \\
\cdots  & \cdots & p_\beta(x_n)(1-p_\beta(x_n))
\end{pmatrix}
\end{align} 
$$

We have $p_\beta(x_i)(1-p_\beta(x_i))\geq0$ hence $W(\beta)$ is semi-definite negative and since $rank(X)=p$, $H(\beta)$ is concave.

It is shown in [@albert1984a] that if additionally there is no complete separation in the training set: ![](images/unique_mle.png){fig-align="center"}

then the MLE exists and is unique.

#### The Newton-Raphson (ie Fisher Scoring) method

In practice the Newton-Raphson method is used to solve the equation:

$$
S(\beta)=\nabla\ell(Y,\beta)=X^T(Y-P_{\beta})=0
$$

Using Taylor expansion of Score $S(\beta)$:

$$
S(\hat\beta) \approx S(\beta^{(k)})+H(\beta^{(k)})(\hat\beta-\beta^{(k)})
$$ and starting from an initial guess of $\beta=\beta_0$, the Newton-Raphson update formula is:

$$
\beta^{(k+1)} = \beta^{(k)} - H^{-1}(\beta^{(k)})S(\beta^{(k)})
$$

We show below a naive implementation of Newton-Raphson method to estimate $\beta$ (also known as Fisher Scoring algorithm in the context of Logistic Regression)

```{r}
#| code-fold: show
# We put the data frame in matrix form
# also adding an intercept
X <- cbind(rep(1, nrow(default_data)),
                          as.matrix(default_data %>% select(balance, income))) 
colnames(X) <-  c("(Intercept)", "balance", "income")
n <- nrow(X)

# We extract the output as vector
Y <- default_data %>% mutate(default = if_else(default=='Yes', 1, 0)) %>% pull(default)


# We set an initial guess for beta and criterion for stopping
beta <- c(0.01, 0.0, 0.0)
nb_iter <- 25
tol <- 1e-4

lr_solve <- function(X, Y, beta, nb_iter, tol){
    for(i in 1:nb_iter){
        # first compute p_beta(X)
        p_beta <- exp(X %*% beta) / (1 + exp(X %*% beta))
        
        # then the Score
        Score_beta <- t(X) %*% (Y-p_beta)
        
        # and the Hessian
        W_beta <- matrix(0, n, n)
        diag(W_beta) <- p_beta*(1-p_beta)
        
        Hessian_beta <- -t(X) %*% W_beta %*% X
        
        # we update beta
        new_beta <- beta - solve(Hessian_beta) %*% Score_beta
        
        # we check for convergence
        if(t(beta-new_beta) %*% (beta-new_beta) < tol){
            return(list(beta = beta, hessian = Hessian_beta, nb_iter = i)) 
        }
        beta <- new_beta
    }
    return(list(beta = beta, hessian = Hessian_beta, nb_iter = i)) 
}

sol <- lr_solve(X, Y, beta, nb_iter, tol)
```

We verify that R `glm()` and our algorithm give the same coefficients:

-   Newton-Raphson:

```{r}
print(sol$beta)
```

-   R `glm()`:

```{r}
glm_bal_inc <- glm(default ~ balance + income,
                 data = default_data,
                 family = "binomial")

print(coef(glm_bal_inc ))

```

In the next sections (Interpretation, Confidence intervals, Tests) we will try to understand the outputs of the `glm()` function from a statistical viewpoint.

#### The Iterative Reweighted Least Square (IRLS) method

There is an equivalent approach to the the Newton-Raphson described in the literature as Iterative Reweighted Least Square (IRLS).

The Newton-Raphson update formula rewrites:

$$
\begin{align}
\beta^{(k+1)} &= \beta^{(k)} - H^{-1}(\beta^{(k)})S(\beta^{(k)}) \\
&=\beta^{(k)} + (X^TW_{\beta^{(k)}}X)^{-1}X^T(Y-P_{\beta^{(k)}}) \\
&= (X^TW_{\beta^{(k)}}X)^{-1}(X^TW_{\beta^{(k)}}X)\beta^{(k)} + (X^TW_{\beta^{(k)}}X)^{-1}X^T(Y-P_{\beta^{(k)}}) \\ 
&= (X^TW_{\beta^{(k)}}X)^{-1}X^TW_{\beta^{(k)}}\left(X\beta^{(k)}+W_{\beta^{(k)}}^{-1}(Y-P_{\beta^{(k)}}) \right) \\
&= (X^TW_{\beta^{(k)}}X)^{-1}X^TW_{\beta^{(k)}}Z_{\beta^{(k)}} \\
\end{align}
$$

where:

$$
Z_{\beta^{(k)}}= X\beta^{(k)}+W_{\beta^{(k)}}^{-1}(Y-P_{\beta^{(k)}})
$$

$\beta^{(k+1)} = (X^TW_{\beta^{(k)}}X)^{-1}X^TW_{\beta^{(k)}}Z_{\beta{(k)}}$ corresponds to the solution of a weighted ($W_{\beta^{(k)}}$) linear regression of $Z_{\beta^{(k)}}$ by $X$. As an exercise you can implement this algorithm.

#### Example where MLE is not finite

To conclude on the numerical aspects, we signal a special and extreme case where the iterative algorithm won't converge. The theoretical aspect is covered in [@albert1984a].

We simulate a perfectly separated data set. Here $X\in \mathbb [-1,1]$ and $Y\in\{0,1\}$:

```{r}
set.seed(1987)
X <- c(runif(n = 50, min = -1, max = 0),
       runif(n = 50, min = 0, max = 1))
Y <- c(rep(0, 50), rep(1, 50))

tbl_separated <- tibble(X,Y)
ggplot(tbl_separated) + geom_point(aes(X, Y))
```

In this setting the iterative algorithm fails to converge and coefficient "saturates" to a high/low value (while it should go to infinite):

```{r}
glm_separated <- glm(Y ~ X,
                     data = tbl_separated,
                     family="binomial")

glm_separated$coef
```

```{r}
separated_fit <- broom::augment(glm_separated, type.predict = "response")
ggplot(separated_fit) +
    geom_point(aes(X, Y)) +
    geom_line(aes(X,.fitted))
```

Now we slightly modify the data set, changing a $y$ observation with $x\in[-1,0]$ from $0$ to $1$.

```{r}
Y1 <- Y
Y1[25] <- 1
tbl_overlap <- tibble(X,Y1)
ggplot(tbl_overlap) + geom_point(aes(X, Y1))
```

The iterative algorithm converges again and the impact on the Scoring function (i.e. $\mathbb{P}[Y=1|X=x)$) and the decision rule (shifting left below $X=O$) is not negligible for a one point change:

```{r}
glm_overlap <- glm(Y1 ~ X,
                     data = tbl_separated,
                     family="binomial")
overlap_fit <- broom::augment(glm_overlap, type.predict = "response")
ggplot(overlap_fit) +
    geom_point(aes(X, Y1)) +
    geom_line(aes(X,.fitted))
```

Nonetheless the case we described is very unlikely to happen in a real life setting, and a good data set exploration should avoid such trap. More details can be found in the document `Separation and Convergence Issues in Logistic Regression.pdf`

### Logistic Regression as a machine learning approach

We have another look at the log-likelihood equation stated before, we have:

$$
\begin{align}
\ell(Y,\beta)=\log L(Y,\beta) &=\sum_{i=1}^n \left(y_i \log(p_{\beta}(x_i))+(1-y_i) \log(1- p_{\beta}(x_i))\right) \\
                              &=-\sum_{i=1}^{n} \ell_{logistic} \left(p_{\beta}(x_i),y_i\right) \\
                              &= -n\hat{\mathrm R}(p_\beta) 
\end{align} 
$$

where $\ell_{logistic}: \{0,1\}\times \{0,1\} \to \mathbb R^+$:

$$
\ell_{logistic}(y,z) = -y\log(z)-(1-y)\log(1-z)=\left\{ \begin{array}{ll} 
    -\log(z) &  \mbox{if } y = 1\cr
    -\log(1-z) &  \mbox{if } y = 0\cr
\end{array} \right.
$$

and $\hat{\mathrm{R}}(p_\beta)$ is the empirical risk on the training set.

Estimating $\beta$ by maximizing the log-likelihood is equivalent to minimizing with respect to $\beta$ the empirical risk of $p_\beta$ for the logistic loss. Note that usually in the context of machine learning and logistic loss, the output $Y$ is relabeled to $\{-1,1\}$








# Interpretation


```{r}
summary(glm_default)
```


Based on this output, the fitted model is (we have re-scaled balance and income for better readability):

$$
\log \bigg( \frac{p_{\beta}(x_i)}{1 - p_{\beta}(x_i)}\bigg) = -1.08 - 0.65(\mathbb{1}_{\mathrm{student}_i=\mathrm{Yes}}) + 5.74(\frac{\mathrm{balance}_i}{1000})+ 0.03(\frac{\mathrm{income}_i}{10000})
$$


## Coefficients

::: {.callout-tip icon="false"}
We cannot interpret the coefficients in the same manner as we interpret coefficients from a linear model, as the outcome is now expressed in "logits":

-   The predicted logit (or as we will see later log-odds) of defaulting for non-students with zero balance and income are $-1.08$.
-   Each one-unit difference in $\frac{\mathrm{balance}}{1000}$ is associated with a difference of 5.74 in the predicted logit of defaulting.
-   Each one-unit difference in $\frac{\mathrm{income}}{10000}$ is associated with a difference of 0.03 in the predicted logit of defaulting.
:::

## Odds

We remind the following relationship:

$$
\mathrm{logit}(p_{\beta}(x))=\log(\frac{p_{\beta}(x)}{1-p_{\beta}(x)})=x^T\beta
$$



The ratio on which we take the logarithm is called odds:

$$
odd_\beta(x) = \frac{p_{\beta}(x)}{1-p_{\beta}(x)}=exp(x^T\beta)
$$

It represents the chance an event occurs ($p_{\beta}(x)$) versus the chance that same event does not occur ($1-p_{\beta}(x)$). 

Odds are an alternative scale to probability for representing chance. 

They arose as a way to express the payoffs for bets. An even bet means that the winner gets paid an equal amount to that staked. 

A 3--1 against bet would pay $\$3$ for every $\$1$ bet, while a 3--1 on bet would pay only $\$1$ for every $\$3$ bet.

For an event $A$ we have $odds(A) \in [0,+\infty[$, and $odds(A) > 1$ if $\mathbf P(A) > 0.5$.

Example: $P(A)= \frac{3}{5}$ is equivalent to $Odds(A) = 1.5$. It means $A$ happens 1.5 more often than its complement Not $A$.

We can also rewrite:

$$
p_\beta(x) = \frac{odds_{\beta}(x)}{1+odds_{\beta}(x)}
$$
To set these ideas, for a variable $x$ in $[-5,5]$, we plot the logistic curve (i.e. the probabilities) together with the odds and the logits (ie log(odds)):

```{r}
# Create w values and transformed values
data_logistic = tibble(x = seq(from = -5, to = 5, by = 0.01)) %>%
                  mutate(probs = exp(x) / (1 + exp(x)), # logistic curve
                         odds = probs / (1 - probs),
                         logits = log(odds))

# View data
# Logistic curve / sigmoid (probabilities)
p1 <-  ggplot(data = data_logistic , aes(x = x, y = probs)) +
  geom_line() +
  theme_light() +
  ylab("Probabilities")

# Exponential curve (odds)
p2 <-  ggplot(data = data_logistic , aes(x = x, y = odds)) +
  geom_line() +
  theme_light() +
  ylab("Odds")

# Linear curve (log-odds)
p3 <-  ggplot(data = data_logistic, aes(x = x, y = logits)) +
  geom_line() +
  theme_light() +
  ylab("Logits=log(Odds)")

library(patchwork)

p1 + p2 + p3
```




::: {.callout-tip icon="false"}
Interpret the coefficients in terms of odds:

-   The coefficient of balance is `r round(as.numeric(coef(glm_default)["balance"]),6)`. Hence an increase of balance by 1000 points increases odds for default by a factor of `r round(exp(as.numeric(coef(glm_default)["balance"])*1000),1)`.
-   The coefficient of income is `r round(as.numeric(coef(glm_default)["income"]),7)`. Hence an increase of income by 10000 points increases odds for default by a factor of `r round(exp(as.numeric(coef(glm_default)["income"])*10000),2)`.
- The coefficient of "being a student" is `r round(as.numeric(coef(glm_default)["studentYes"]),3)`. Hence being a student decreases odds for default by a factor of `r round(exp(as.numeric(coef(glm_default)["studentYes"])),2)`.
:::

## Odds ratio

For two observations $x$ and $\tilde{x}$ we define odds ratio as:

$$
OR(x,\tilde{x})=\frac{odds(x)}{odds(\tilde{x})}
$$
Odds ratio are used to compare probabilities between two observations:

-   $OR(x,\tilde{x}) = 1 \Leftrightarrow p(x)=p(\tilde{x})$ 
-   $OR > 1 \Leftrightarrow p(x)>p(\tilde{x})$ 
-   $OR < 1 \Leftrightarrow p(x)<p(\tilde{x})$ 

They are also used to measure the impact of a predictor:

$$
OR(x,\tilde{x})=\exp(\beta_1(x_1-\tilde{x_1}))\cdots exp(\beta_p(x_p-\tilde{x_p}))
$$
Choosing $(x,\tilde{x})$ differing by only one predictor $x_j$:

$$
OR(x,\tilde{x})=\exp(\beta_j(x_j-\tilde{x_j}))
$$

In other words, $exp(\beta_j)$ is the odds ratio associated with a one-unit increase in the $x_j$.

More on odds ratio interpretation can be found [here](https://stats.oarc.ucla.edu/other/mult-pkg/faq/general/faq-how-do-i-interpret-odds-ratios-in-logistic-regression/). 



# Inference

## Asymptotic properties of MLE

It can be proven that under certain assumptions (see for example @gourieroux1981 or @fahrmeir1986), the Maximum Likelihood Estimator has the following asymptotic properties:

$$
\hat\beta \xrightarrow[] {p} \beta \textrm{, as n} \to \infty
$$

and

$$
\sqrt n(\hat\beta-\beta) \xrightarrow[]{\mathcal L} \mathcal N(0,\mathcal I(\beta)^{-1})\textrm{, as n} \to \infty
$$ 
where:

$$
\mathcal I(\beta) = -\mathbb{E}[\nabla^2\ell(Y,\beta)]=-\frac{1}{n}\nabla^2\ell(Y,\beta)=\frac{1}{n}X^T W_\beta X
$$

where $\mathcal I(\beta)$ is the Fisher information matrix. In the case of Logistic Regression, Fisher information matrix equals the Observed information matrix.

The asymptotic property rewrites:

$$
(\hat\beta-\beta)^Tn\mathcal I(\beta)(\hat\beta-\beta) \xrightarrow[]{\mathcal L} \chi_p^2
$$

As $\mathcal I(\beta)$ is unknown we use instead $\mathcal I(\hat\beta)=\frac{1}{n}X^T W_\hat\beta X$. Since $\hat\beta \xrightarrow[] {p} \beta$ and $p_\beta$ continuous in $\beta$ it can be shown (Slutsky) that:

$$
(\hat\beta-\beta)^TX^T W_{\hat\beta} X(\hat\beta-\beta) \xrightarrow[]{\mathcal L} \chi_p^2
$$

Or equivalently:

$$
\hat\beta -\beta \xrightarrow[]{\mathcal L} \mathcal N(0,\mathcal ( X^T W_{\hat\beta} X )^{-1})
$$


## Wald statistics

### Confidence intervals 

Using the preceding asymptotic properties we can derive confidence interval and tests for the coefficients $\beta_j$, $j =1,\cdots,p$ of the model:

$$
\frac{\hat\beta_j-\beta_j}{\hat \sigma_j} \xrightarrow[]{\mathcal L} \mathcal N(0,1)
$$
where $\hat \sigma_j^2= s.e.(\hat\beta_j)^2$ denotes the $j-th$ term of $( X^T W_{\hat\beta} X )^{-1}$ diagonal.

The typical formula for a $1-\alpha$ confidence interval is:

$$
\hat\beta_j \pm z_{1-\alpha/2} \hat \sigma_j
$$
where $z_{1-\alpha/2}$ is the $(1-\alpha/2)$ quantile of the standard normal distribution.

Going further, the asymptotic properties of MLE also allow to test the "statistical significance" of each coefficient in the model, the Wald test. 

Denoting: $\textrm{H}_0\textrm{: } \beta_j=0$ and $\textrm{H}_1\textrm{: } \beta_j \neq 0$ we have under $\textrm{H}_0$:

$$
\frac{\hat\beta_j}{\hat \sigma_j} \xrightarrow[]{\mathcal L} \mathcal N(0,1)
$$
We will reject $\textrm{H}_0$ at level $\alpha$ if the absolute of the observed value  $\frac{\hat\beta_j}{\hat \sigma_j}$ (denoted in `glm` output as `z value`) is above the $(1-\alpha/2)$ quantile of the standard normal distribution.




```{r}
#| code-fold: show
glm_bal_inc <- glm(default ~ balance + income,
                 data = default_data,
                 family = "binomial")
summary(glm_bal_inc)
```
Said differently, we reject $\textrm{H}_0$ at level $\alpha$ when  $p = \mathbf P(|z|>|\frac{\hat\beta_j}{\hat \sigma_j}|)<\alpha$.

$p$ is called the p-value.

The output of `glm` in `R` shows:

* $\hat\beta_j$ as `Estimate`,

* $\hat \sigma_j$ as `Std. Error`,

* the absolute of the observed test statistic  $\frac{\hat\beta_j}{\hat \sigma_j}$ as `z value`,

* and the p-value as `Pr(>|z|)`

```{r}
(1-pnorm(4.174))*2
```


Using the hessian matrix obtained before as a side product of the Newton-Raphson algorithm, we retrieve the same values as `glm` for $\hat \sigma_j$:

```{r}
#| code-fold: show
std_errors <- sqrt(diag(solve(-as.matrix(sol$hessian))))
std_errors
```

The `R` command to get confidence interval of estimators based on Wald statistic is the following (by default $\alpha=5\%$)

```{r}
#| code-fold: show
confint.default(glm_bal_inc)
```
We can retrieve it manually using coefficient estimate and standard deviation:

```{r}
#| code-fold: show
output_bal_inc = summary(glm_bal_inc)$coefficients
bal_std_estimate <- output_bal_inc[2,1]
bal_std_error <- output_bal_inc[2,2]

# upper bound for beta(balance) at 5%
upper <- bal_std_estimate + 1.96 * bal_std_error

# lower bound for beta(balance) at 5%
lower <- bal_std_estimate - 1.96 * bal_std_error
(bal_confint <- c(lower, upper))
```

```{r}
#| code-fold: show
confint(glm_bal_inc)
```

https://stats.stackexchange.com/questions/5304/why-is-there-a-difference-between-manually-calculating-a-logistic-regression-95

### Tests on model coefficients

Based on the same idea, it is possible to test for the nullity of a subset of the model coefficients.

Denoting: $\textrm{H}_0\textrm{: } \beta_1=\cdots=\beta_q=0$, $\textrm{H}_1\textrm{: } \exists j \in \{1,\cdots,q \} \textrm{ }|\textrm{ }\beta_j \neq 0$, $\hat\beta=(\hat\beta_1,\cdots,\hat\beta_p)$ the MLE and $\hat\beta_{1:q}=(\hat\beta_1,\cdots,\hat\beta_q)$ the vector of first $q$ parameters.

We have under $\textrm{H}_0$:

$$
\hat\beta_{1:q}^T( X^T W_{\hat\beta} X )^{-1}_{1:q}\hat\beta_{1:q} \xrightarrow[]{\mathcal L} \chi_q^2
$$
where $(X^T W_{\hat\beta} X )^{-1}_{1:q}$ is the $q \times q$ upper left block matrix extracted from the inverse of hessian.

We will reject $\textrm{H}_0$ at level $\alpha$ if the observed value  $\hat\beta_{1:q}^T( X^T W_{\hat\beta} X )^{-1}_{1:q}\hat\beta_{1:q}$ is above the $1-\alpha$ quantile of the $\chi_q^2$ distribution.


We show below the Wald tests for each coefficient in the model using `summary`:
```{r}
#| code-fold: show
#| # Testing all coefficients
summary(glm_default)
```

These tests can be also performed in `R` using `car::Anova` or `aod::wald.test` routines. In particular when categorical variables have more than two levels these functions allow to test each variables as a whole (vs coefficient by coefficient when using `summary`)

```{r}
#| code-fold: show
#| # Testing all coefficients
car::Anova(glm_default, type=3, test.statistic= "Wald")
```
We can retrieve these outputs manually:
```{r}
sum_default <- summary(glm_default)
beta_income <- sum_default$coefficients[4,1]
stdev_income <- sum_default$coefficients[4,2]

wald <- beta_income ^ 2 / stdev_income ^ 2

1-pchisq(wald, df = 1)

z_val <- sum_default$coefficients[4,3]

2*(1-pnorm(z_val))
```


```{r}
#| code-fold: show
# Testing the income coefficient (Terms = 4)
aod::wald.test(b = coef(glm_default), Sigma = vcov(glm_default), Terms = 4)

```
With all routines, the p-value for the income coefficient is $0.71$ validating the null hypothesis.

Using `Terms` or `L` parameters in `aod::wald.test` it is also feasible to test the null hypothesis for a subsets of parameters:

```{r}
#| code-fold: show
# Testing the income coefficient (Terms = 4)
aod::wald.test(b = coef(glm_default), Sigma = vcov(glm_default), Terms = 1:3)
```
The null hypothesis is rejected for the model with balance and student.


There are known issues with the Wald test:

-   It is not invariant to re-reparametrisation. In the case of logistic regression we have a nonlinear model where the individual parameters are not of foremost interest, but rather odds ratios for instance, which are nonlinear transforms of the parameters.

-   Hauck and Donner (1977) have shown that the Wald test has undesirable properties for logistic regression. In particular when $\hat \beta_j \to \infty$ (e.g. in case of separation or quasi separation), it is likely that $\hat\sigma_j\to \infty$. The result can be that the Wald statistic tends to zero as the distance between the parameter estimate and the null value increases (null hypothesis gets more and more wrong).

So in the context of Logistic Regression (and GLM), while Wald statistics are usually reported by statistical routines, the likelihood ratio or deviance-based tests are often favored.


## Likelihood ratio tests

It is possible to test for the nullity of a subset of the model coefficients using Likelihood Ratio statistics.

Denoting: $\textrm{H}_0\textrm{: } \beta_1=\cdots=\beta_q=0$, $\textrm{H}_1\textrm{: } \exists j \in \{1,\cdots,q \} \textrm{ }|\textrm{ }\beta_j \neq 0$, and $\hat\beta=(\hat\beta_1,\cdots,\hat\beta_p)$ the MLE, we have under $\textrm{H}_0$:

$$
-2\left(\ell_{\textrm{H}_0}(Y,\hat\beta_{\textrm{H}_0})-\ell(Y,\hat\beta)\right)\xrightarrow[]{\mathcal L} \chi_q^2
$$
where $\ell_{\textrm{H}_0}(Y,\hat\beta_{\textrm{H}_0})$ is the log-likelihood of:

$$
\mathrm{logit}(p_{\beta}(X))=x_{q+1}\beta_{q+1}+\cdots+x_{n}\beta_{n}
$$


Consider two models, a larger model with $l$ parameters and likelihood $L_L$ and a smaller model with $s$ parameters and likelihood $L_S$, where the smaller model represents a subset of the larger model. Typically the smaller model is equivalent to the large model where we have imposed:

$$
\textrm{H}_0\textrm{: } \ \beta_j = \ldots  = \beta_{j+r}  = 0
$$
Likelihood Ratio tests on variables may be performed in `R` using `car::Anova`:

```{r}
#| code-fold: show
#| # Testing all coefficients
car::Anova(glm_default, type=3, test.statistic= "LR")
```
Using base `R` `anova` it is also possible to test subsets of variables and in particular individual variables within the "full" model:

```{r}
#| code-fold: show
glm_wo_student <- glm(default ~ balance + income,
                 data = default_data,
                 family = "binomial")
glm_wo_balance <- glm(default ~ student + income,
                 data = default_data,
                 family = "binomial")
glm_wo_income <- glm(default ~ student + balance,
                 data = default_data,
                 family = "binomial")

```

```{r}
#| code-fold: show
anova(glm_wo_student, glm_default, test = "LRT")
```
We can retrieve this result manually:

```{r}
#| code-fold: show
LRT <- 2 * (logLik(glm_default)-logLik(glm_wo_student))

1 - pchisq(LRT, df = 1)
```



```{r}
#| code-fold: show
anova(glm_wo_balance, glm_default, test = "LRT")
```

```{r}
#| code-fold: show
anova(glm_wo_income, glm_default, test = "LRT")
```

The deviance defined as $D=-2 \ell$ is often reported by statistical software in place of log-likelihood. A large likelihood corresponding to a small deviance. 

A better coverage of tests in the context of Logistic Regression can be found [here](https://stats.oarc.ucla.edu/other/mult-pkg/faq/general/faqhow-are-the-likelihood-ratio-wald-and-lagrange-multiplier-score-tests-different-andor-similar/) or in [@hosmer2013]. See also [here](https://stats.oarc.ucla.edu/r/dae/logit-regression/) for a data analysis using `R` and [here](https://stats.stackexchange.com/questions/86351/interpretation-of-rs-output-for-binomial-regression) for a detailed description of the outputs of `glm()`.

# Variable selection, model assessment

We have seen in the last section how to compare two or more nested Logistic regression models (typically a reduced model with less variables than a reference or full model). 

Real life data sets usually contain a large number of predictors or inputs. As their number grow, the analyst will have to analyse a growing number of possible models or variable combinations (typically $2^p$ where $p$ denotes the number of predictors). Furthermore as the number of predictors grow the models might overfit the training set, causing a deterioration of prediction error.

Variable selection is a critical aspect of building Logistic Regression models and prediction models in general. This process seeks to strike a delicate balance between the bias-variance trade-off, aiming to find models that are both parsimonious and predictive. Parsimony, in this context, implies the selection of a minimal set of predictor variables that still provides an accurate representation of the data, avoiding overfitting while enhancing model generalization. 

This is a complex subject that we will not cover in depth in this course.

For a better coverage, chapter 7 `Model Assessment and Selection` of @hastie2009 discusses in depth the interplay between bias, variance and model complexity. Chapter 6 `Linear Model Selection and Regularization` of @islr2021 discusses methods to automatically perform variable selection in the context of linear models. 

Various methods and criteria are employed for variable selection. In this section we consider some methods for selecting subsets of predictors: this include best subset, stepwise model selection procedures and penalization or shrinkage. We use the Agriculture Farm Lending data set as an example.


## Best subset

One common approach is the "best subset" method, which evaluates all possible combinations of predictor variables, resulting in $2^p-1$ models for p predictors, and selects the one that optimizes a specified criterion, such as the Akaike Information Criterion (AIC) or Bayesian Information Criterion (BIC). It is computationally intensive and usually restricted to low dimension data sets.

******
**Algorithm**:  Best subset selection

******
1.  For $k=1,\cdots,p$:
    (a) Fit all ${p\choose k}$ models that contain exactly $k$ predictors.
    (b) Pick the best among these ${p\choose k}$ models, and call it $\mathcal M_k$. In the case of Logistic Regression, best usually means largest log-likelihood or min deviance.
2.  Select a single best model from among $\mathcal M_1,\cdots,\mathcal M_p$ using a criterion. Usually the prediction error on a validation set, AIC, BIC... 
******

Given $|\mathcal M|$ we define the two following criteria which represent two way of penalizing the maximised log-likelihood $\ell(Y,\hat\beta)$:

$$
\textrm{AIC}(\mathcal M)=-2\ell(Y,\hat\beta)+2|\mathcal M|
$$

and

$$
\textrm{BIC}(\mathcal M)=-2\ell(Y,\hat\beta)+|\mathcal M|\log(n)
$$
The package `bestglm` allows best subset selection up to $15$ variables, by default it uses BIC. We use the Default data set to illustrate because Agriculture Farm Lending has around $30$ variables:

```{r}
#| code-fold: show
library(bestglm)

default_best <- as.data.frame(default_data %>% mutate(Y=if_else(default=='Yes', 1, 0)) %>% select(-default))
# p must be < 15 for GLM
# Error in bestglm(as.data.frame(don_desbois_quanti), family = binomial) :
# p = 22. must be <= 15 for GLM.

mod_sel <- bestglm(default_best, family = binomial)
mod_sel
```




## Stepwise Logistic Regression

Alternatively, stepwise methods, including forward and backward selection, iteratively add or remove variables based on the chosen criterion. These approaches, while useful, can be also computationally intensive, especially for high-dimensional datasets. 

******
**Algorithm**:  Forward stepwise selection

******
1.  For $k=1,\cdots,p$:
    (a) Consider all $p+1-k$ models that augment the predictors in $\mathcal M_k$ with one additional predictor.
    (b) Pick the best among these $p+1-k$ models, and call it $\mathcal M_{k+1}$. In the case of Logistic Regression, best usually means largest log-likelihood or min deviance.
2.  Select a single best model from among $\mathcal M_1,\cdots,\mathcal M_p$ using a criterion. Usually the prediction error on a validation set, AIC, BIC... 

******

or 

******
**Algorithm**:  Backward stepwise selection

******
1. Let $\mathcal M_p$ denote the full model, which contains all $p$ predictors.
1.  For $k=p,\cdots,1$:
    (a) Consider all $k$ models that contain all but one of the predictors in $\mathcal M_k$, for a total of $k-1$ predictors.
    (b) Pick the best among these $k$ models, and call it $\mathcal M_{k-1}$. In the case of Logistic Regression, best usually means largest log-likelihood or min deviance.
2.  Select a single best model from among $\mathcal M_1,\cdots,\mathcal M_p$ using a criterion. Usually the prediction error on a validation set, AIC, BIC... 

******

Below we use forward stepwise selection based on the AIC criterion to select variables in the Agriculture Farm Lending data set:

```{r}
#| code-fold: show

# package used to import spss file
library(foreign)

don_desbois <- read.spss("../data/Agriculture Farm Lending/desbois.sav",
                       to.data.frame = TRUE) %>% as_tibble()
don_desbois <- don_desbois %>%
    mutate(Y = as.factor(if_else(DIFF=='healthy', 0, 1))) %>%
    dplyr::select(-DIFF)

# Due to outliers some predictions "saturate"  to 0 or 1
# Warning: glm.fit: fitted probabilities numerically 0 or 1 occurredWarning: glm.fit: fitted probabilities numerically 0 or 1 occurred

# winsorizing financial ratios removes the issue
don_desbois_winsorized <- don_desbois %>%
    mutate(across(r1:r37, ~ DescTools::Winsorize(.x , probs = c(0.025, 0.975))))
# data_afl <- don_desbois_winsorized

data_afl <- don_desbois

#define intercept-only model
intercept_only <- glm(Y ~ 1, data=data_afl, family="binomial")

#define model with all predictors
# In Desbois only financial ratios are used
all <- glm(Y ~ ., data=data_afl %>% select(Y, starts_with('r')), family="binomial")

# We use all variables
# all <- glm(Y ~ ., data=data_afl , family="binomial")
```

```{r, warning=FALSE}
#| code-fold: show
# perform forward stepwise regression
forward_aic <- step(intercept_only, direction='forward', test = 'LRT', scope=formula(all), k=2, trace = FALSE)
summary(forward_aic)
```

We perform manually a likelihood-ratio-test-based forward selection (as described in Desbois) using only financial ratios as predictors.



```{r, warning = FALSE}
#| code-fold: show
all <- glm(Y ~ ., data=data_afl %>% select(Y, starts_with('r')), family="binomial")

first_step <- add1(intercept_only, scope = formula(all), test = "LRT")
first_step <- first_step %>% 
    tibble() %>%
    add_column(variable=row.names(first_step)) %>% 
    arrange(desc(LRT))
first_step
```
`r1` is the most significant variable in the first step

```{r, warning = FALSE}
#| code-fold: show
second_step <- add1(update(intercept_only, ~. + r1), scope = formula(all), test = "LRT")
second_step <- second_step %>% 
    tibble() %>%
    add_column(variable=row.names(second_step)) %>% 
    arrange(desc(LRT))
second_step
```
`r21` is the most significant variable in the second step (it differs from the article where r32 is selected (p70, the r32 in R is the same  as in article (129.65) but r21 and r14 have higher LRT)).


```{r, warning = FALSE}
#| code-fold: show
third_step <- add1(update(intercept_only, ~. + r1 + r21), scope = formula(all), test = "LRT")
third_step <- third_step %>% 
    tibble() %>%
    add_column(variable=row.names(third_step)) %>% 
    arrange(desc(LRT))
third_step
```

`r14` is the most significant variable in the third step.

```{r, warning = FALSE}
#| code-fold: show
fourth_step <- add1(update(intercept_only, ~. + r1 + r21 + r14), scope = formula(all), test = "LRT")
fourth_step <- fourth_step %>% 
    tibble() %>%
    add_column(variable=row.names(fourth_step)) %>% 
    arrange(desc(LRT))
fourth_step
```
`r17` is the most significant variable in the fourth step.

```{r, warning = FALSE}
#| code-fold: show
fifth_step <- add1(update(intercept_only, ~. + r1 + r21 + r14 + r17), scope = formula(all), test = "LRT")
fifth_step <- fifth_step %>% 
    tibble() %>%
    add_column(variable=row.names(fifth_step)) %>% 
    arrange(desc(LRT))
fifth_step
```

`r24` is the most significant variable in the fifth step.

```{r, warning = FALSE}
#| code-fold: show
sixth_step <- add1(update(intercept_only, ~. + r1 + r21 + r14 + r17 + r24), scope = formula(all), test = "LRT")
sixth_step <- sixth_step %>% 
    tibble() %>%
    add_column(variable=row.names(sixth_step)) %>% 
    arrange(desc(LRT))
sixth_step
```
`r11` is the most significant variable in the sixth step.

Equivalently the `step` routine produces the same results:

```{r, warning=FALSE}
#| code-fold: show
# perform forward stepwise regression
forward_aic <- step(intercept_only, direction='forward', test = 'LRT', scope=formula(all), k=2, trace = FALSE)
summary(forward_aic)
```

We now perform manual likelihood-ratio-test-based forward selection with override to converge with Desbois article (i.e. selecting the same variables as in Desbois if R diverge).


```{r, warning = FALSE}
#| code-fold: show
first_step_article <- add1(intercept_only, scope = formula(all), test = "LRT")
first_step_article <- first_step_article %>% 
    tibble() %>%
    add_column(variable=row.names(first_step_article)) %>% 
    arrange(desc(LRT))
head(first_step, 4)
```
`r1` is the most significant variable in the first step

```{r, warning = FALSE}
#| code-fold: show
second_step_article <- add1(update(intercept_only, ~. + r1), scope = formula(all), test = "LRT")
second_step_article <- second_step_article %>% 
    tibble() %>%
    add_column(variable=row.names(second_step_article)) %>% 
    arrange(desc(LRT))
head(second_step,4)
```

In the article `r32` is the most significant variable in the second step: we choose `r32`instead of `r21`. We have exactly the same LRT as in the article 


```{r, warning = FALSE}
#| code-fold: show
third_step <- add1(update(intercept_only, ~. + r1 + r32), scope = formula(all), test = "LRT")
third_step <- third_step %>% 
    tibble() %>%
    add_column(variable=row.names(third_step)) %>% 
    arrange(desc(LRT))
head(third_step, 4)
```

Like in the article `r14` is the most significant variable in the third step (with same LRT).

```{r, warning = FALSE}
#| code-fold: show
fourth_step <- add1(update(intercept_only, ~. + r1 + r32 + r14), scope = formula(all), test = "LRT")
fourth_step <- fourth_step %>% 
    tibble() %>%
    add_column(variable=row.names(fourth_step)) %>% 
    arrange(desc(LRT))
head(fourth_step, 4)
```
In the article `r17` is the most significant variable in the fourth step: : we choose `r17`instead of `r18`. 

```{r, warning = FALSE}
#| code-fold: show
fifth_step <- add1(update(intercept_only, ~. + r1 + r32 + r14 + r17), scope = formula(all), test = "LRT")
fifth_step <- fifth_step %>% 
    tibble() %>%
    add_column(variable=row.names(fifth_step)) %>% 
    arrange(desc(LRT))
head(fifth_step,4)
```

Like in the article `r36` is the most significant variable in the fifth step (with same LRT)

```{r, warning = FALSE}
#| code-fold: show
sixth_step <- add1(update(intercept_only, ~. + r1 + r32 + r14 + r17 + r36), scope = formula(all), test = "LRT")
sixth_step <- sixth_step %>% 
    tibble() %>%
    add_column(variable=row.names(sixth_step)) %>% 
    arrange(desc(LRT))
head(sixth_step, 4)
```
Like in the article `r12` is the most significant variable in the sixth step (with same LRT). The routine (`SPSS`) in the article stops with this six variables. 

![](images/desbois_forward_stepwise.png){fig-align="center" width="350"}



R performs automated AIC/BIC based variable selections using (step)

```{r, warning=FALSE}
#| code-fold: show
forward_bic <- step(intercept_only, direction='forward', test = 'LRT', scope=formula(all), k=log(nrow(don_desbois)), trace = FALSE)
summary(forward_bic)
```

```{r, warning=FALSE}
#| code-fold: show
backward_aic <- step(all, direction='backward', test = 'LRT', scope=formula(all), k=2, trace = FALSE)
summary(backward_aic)
```

```{r, warning=FALSE}
#| code-fold: show
backward_bic <- step(all, direction='backward', test = 'LRT', scope=formula(all), k=log(nrow(don_desbois)), trace = FALSE)
summary(backward_bic)
```


```{r}
#define intercept-only model
intercept_only_w <- glm(Y ~ 1, data=don_desbois_winsorized, family="binomial")

#define model with all predictors
all_w <- glm(Y ~ ., data=don_desbois_winsorized, family="binomial")

#perform forward stepwise regression AIC
forward_aic_w <- step(intercept_only_w, direction='forward', scope=formula(all_w), k=2, trace = FALSE)

#perform forward stepwise regression BIC
forward_bic_w <- step(intercept_only_w, direction='forward', scope=formula(all_w), k=log(nrow(don_desbois_winsorized)), trace = FALSE)
```


```{r}
summary(forward_aic_w)
```




## Introducing penalized logistic regressions

As an alternative, penalized regression techniques like the Lasso (Least Absolute Shrinkage and Selection Operator) or to a lesser extent Ridge offer an efficient means of variable selection by introducing a penalty term in the Logistic Regression estimation algorithm. In particular Lasso promotes the sparsity of coefficients and automatically selects relevant predictors while shrinking others to zero. 

We remind that to estimate Logistic Regression parameters, we maximized in $\beta$ the log-likelihood:

$$
\ell(Y,\beta)=\log L(Y,\beta) =\sum_{i=1}^n \left(y_i \log(p_{\beta}(x_i))+(1-y_i) \log(1- p_{\beta}(x_i))\right)
$$
In the context of penalized Logistic Regression the idea is to minimize in $\beta$:

$$
-\ell(Y,\beta)+\lambda_2\lVert\beta\rVert_2 + \lambda_1\lVert\beta\rVert_1
$$
where $\lambda_1, \lambda_2$ are two constants.

The undelying idea is to constraint or shrink the size of the coefficients estimates.

When $\lambda_1>0, \lambda_2=0$ we are using a Lasso penalty.

When $\lambda_1=0, \lambda_2>0$ we are using a Ridge penalty.

When $\lambda_1>0, \lambda_2>0$ we are using an Elastic Net penalty.

Without entering to much details in this section, Ridge regression does a proportional shrinkage while Lasso truncates the coefficients at zero.

![](images/lasso_classic_graph.png){fig-align="center" width="350"}

Lasso Ridge using Mixture

```{r}
data_mixture_example <- readRDS("../1_Scoring_and_Logistic_Regression/data_mixture_example.rds")
y = as.numeric(data_mixture_example$Y)-1
X = as.matrix(data_mixture_example %>% select(x1,x2))
```


Some intuition behind Ridge shrinkage:

```{r}
grid <- expand.grid(x1 = seq(-3.5, 3.5, .05), x2 = seq(-3.5, 3.5, .05)) %>% as_tibble()
# Prediction function used to classify areas on the grid and imply the decision boundary
LL  <- function(b0, b1, b2){
  beta <-  c(b1, b2)
  return( sum(-y*log(1 + exp(-(b0+X%*%beta))) - 
  (1-y)*log(1 + exp(b0+X%*%beta))))
}

LL_V <- Vectorize(LL)

grid <- grid %>% mutate(x0= 1, LL = LL_V(x0, x1, x2))

xc <- 0
yc <- 0
r <- 0.4

ggplot(grid) + 
    geom_contour(aes(x = x1, y = x2, z = LL, colour = after_stat(level)), binwidth = 10) +
    scale_color_viridis_c(option="H") +
    annotate("path",
             x=xc+r*cos(seq(0,2*pi,length.out=100)),
             y=yc+r*sin(seq(0,2*pi,length.out=100))) +
    coord_fixed()
```

Some intuition behind Lasso shrinkage:

```{r}
grid <- expand.grid(x1 = seq(-3.5, 3.5, .05), x2 = seq(-3.5, 3.5, .05)) %>% as_tibble()
# Prediction function used to classify areas on the grid and imply the decision boundary
grid <- grid %>% mutate(x0= 1, LL = LL_V(x0, x1, x2))

h <- 0.4

ggplot(grid) + 
    geom_contour(aes(x = x1, y = x2, z = LL, colour = after_stat(level)), binwidth = 10) +
    scale_color_viridis_c(option="H") +
    annotate("segment", x=-h, xend=0, y=0, yend=h, col = 'black') +
    annotate("segment", x=0, xend=h, y=h, yend=0, col = 'black') +
    annotate("segment", x=h, xend=0, y=0, yend=-h, col = 'black') +
    annotate("segment", x=0, xend=-h, y=-h, yend=0, col = 'black') +
    coord_fixed()
```

In R the package `glmnet` implements the Lasso, Ridge and Elastic Net penalties in particular for Logistic Regression:

For example below the lasso "path" of coefficients when increasing $\lambda$ (from right to left).

```{r}
library(glmnet)
library(glmnetUtils)

Y <- don_desbois %>% pull(Y)
desbois_lasso <- glmnetUtils::glmnet(Y ~ ., data=don_desbois, family="binomial", alpha=1)

plot(desbois_lasso)

# X <- glmnet::makeX(train = don_desbois %>% dplyr::select(-Y), sparse = TRUE)
# desbois_lasso <- glmnet::cv.glmnet(X, Y, data=don_desbois, family="binomial", alpha=1, type.measure = "auc")
```
Note that the penalized regression solutions are sensitive to the scaling of the inputs, and so one normally standardizes the inputs before solving. 

We can visualise for each $\lambda$ the value of coefficients:

```{r}
lasso_result <- as_tibble(as.matrix(cbind(desbois_lasso$lambda, t(desbois_lasso$beta))))
names(lasso_result) <-  c("lambda", row.names(desbois_lasso$beta))
lasso_result
```

`glmnet` also provides an automatic and efficient procedure to select $lambda$ implementing K-fold Cross-Validation (see next paragraph):

```{r}
desbois_lasso_cv <- glmnetUtils::cv.glmnet(Y ~ ., data=don_desbois, family="binomial", alpha=1, type.measure = "auc")
lasso_coeffs <- coef(desbois_lasso_cv, s = "lambda.1se")
data.frame(name = lasso_coeffs@Dimnames[[1]][lasso_coeffs@i + 1], coefficient = lasso_coeffs@x)
```


## Model assessment

We have seen in the first lesson that in the context of statistical learning the approach was to estimate a classifier or Score with the best possible Risk or any other metric such as the AUC of a ROC curve. The final goal being to predict unobserved outputs given unobserved inputs having trained a classifier using the data at hands.

To assess how well the classifier or Score will "generalize" to new data, a practical approach is to split the data set into a training set and a test set or validation set. The training set being used to learn the classifier or Score, the validation set to estimate the generalization error of the classifier.

Two principal approaches are used: the Hold-out approach and the K-fold Cross-validation approach.

### Hold-out approach

It consists in splitting the data set into:

* a learning or training set used to train the classifier or the Score ; 
* a validation or test set used to estimate the empirical risk of the classifier or any other metric (ROC curve, AUC).

******
**Algorithm**:  Hold-out approach

******
1.  Using a partition training/validation $\{\mathcal T, \mathcal V\}$ of the data set $\mathcal D$:
    (a) Fit the classifiers $f_1\cdots,f_m$ on $\mathcal T$
    (b) Compute  the empirical risk $\hat{\mathrm R}(f) = \frac{1}{n_{\mathcal V}} \sum_i \ell(y_i,f_m(x_i))$ or any other metric on the validation set $\mathcal V$
2.  Select a single best model $f_{m^*}$ with respect to the empirical risk or metric
******

The main drawback using a single split of data is the possible variability of empirical risk, which is more pregnant when the data set size reduces. Against this issue an alternative approach is to repeat this process on multiple splits of the data.

### K-fold Cross-Validation approach

It consists in splitting randomly the data set into $K$ blocks of folds then repeating $K$ times the Hold-out approach, each time using a different block as validation set.


******
**Algorithm**:  K-fold Cross-Validation approach

******
1.  Using a random partition in $K$ blocks $\{\mathcal D_1, \cdots,\mathcal D_K\}$ of the data set $\mathcal D$
1.  For $k=1,\cdots,K$
    (a) $\{\mathcal T_k, \mathcal V_k\}$ with $\mathcal V_k=\mathcal D_k$ and $\mathcal T_k=\mathcal D\setminus\mathcal D_k$
    (a) Fit the classifiers $f_1\cdots,f_m$ on $\mathcal T_k$
    (b) Compute  the empirical risk $\hat{\mathrm R}(f) = \frac{1}{n_{\mathcal V_k}} \sum_i \ell(y_i,f_m(x_i))$ or any other metric on the validation set $\mathcal V_k$
2.  Select a single best model $f_{m^*}$ using the average empirical risk or metric
******

A variant is to first split the data set into a training and validation set. Perform K-fold Cross-Validation on the training set, for example to select some classifier hyper parameter (number of variables or model specification in logistic regression, penalty, etc), then assess the best "optimized" models on the validation set.

Another variant is to repeat the K-fold CV 5 or 10 times to improve the accuracy of the estimated performance and provide and estimate on its variability.

This [book chapter](https://bradleyboehmke.github.io/HOML/process.html) gives a practical overview on these methods (how to implement it in R) and also gives references discussing the validity and limitations of the methods.


## Exercise

Using the Agriculture Farm Lending data set or R package `rsample` and inspiring from this [book chapter](https://bradleyboehmke.github.io/HOML/process.html):

* briefly explore the data set (the full case study is presented in the `root,+Directeur+de+la+revue,+351-1336-1-CE.pdf`)

```{r}
#| code-fold: show
# package used to import spss file
library(foreign)

don_desbois <- read.spss("../data/Agriculture Farm Lending/desbois.sav",
                       to.data.frame = TRUE) %>% as_tibble()
don_desbois <- don_desbois %>%
    mutate(Y = as.factor(if_else(DIFF=='healthy', 0, 1))) %>%
    dplyr::select(-DIFF)

glimpse(don_desbois)
```

```{r}
#| code-fold: show
vars_quanti <- names(don_desbois %>% select_if(is.numeric))
for(var in vars_quanti){
    var <- as.name(var)
    print(ggplot(don_desbois %>%
                     # filter(r17 <0.14) %>% 
                     mutate(Y = as.numeric(Y)-1), aes(x=!! var,y = Y)) +
        geom_jitter(height = 0.1, width = 0) +
        geom_smooth(method = "glm", 
                    formula = y ~ x,
                    method.args = list(family = "binomial"), 
                    se = FALSE,
                    col = "dodgerblue"))
}
```
At first sight there seem to be outliers in the financial ratios values (we can deal with that )

```{r}
#| code-fold: show
don_desbois_winsorized <- don_desbois %>%
    mutate(across(r1:r37, ~ DescTools::Winsorize(.x , probs = c(0.00, 0.975))))

vars_quanti <- names(don_desbois_winsorized %>% select_if(is.numeric))
for(var in vars_quanti){
    var <- as.name(var)
    print(ggplot(don_desbois_winsorized %>%
                     # filter(r17 <0.14) %>% 
                     mutate(Y = as.numeric(Y)-1), aes(x=!! var,y = Y)) +
        geom_jitter(height = 0.1, width = 0) +
        geom_smooth(method = "glm", 
                    formula = y ~ x,
                    method.args = list(family = "binomial"), 
                    se = FALSE,
                    col = "dodgerblue"))
}
```


```{r, message = FALSE}
#| code-fold: show
vars_quali <- names(don_desbois %>% select_if(is.factor) %>% select(-Y))
for(var in vars_quali){
    var <- as.name(var)
    print(ggplot(don_desbois %>% 
                     group_by(!!var, Y) %>% 
                     summarize(count = n()) %>% 
                     ungroup()) +
        geom_bar(aes(x = Y, y = count, fill = !!var), position="dodge",stat="identity"))
}
```

```{r, warning = FALSE, message = FALSE}
#| code-fold: show
class_width <- 0.01
(don_desbois_binned <- don_desbois %>%
    mutate(r17_bins = cut(r17, breaks = seq(0, 0.2, class_width),
                              right = FALSE, dig.lab = 4, include.lowest = TRUE),
           min = floor(r17 / class_width) * class_width,
           max = if_else(r17 == 0 , 1, 
                         # customers with 0$ balance should belong to [0, width) class
                         # or be excluded
                         ceiling(r17 / class_width))  * class_width) %>% 
    group_by(r17_bins, min, max, Y) %>% 
    summarize(n=n()) %>% 
    ungroup() %>% 
    pivot_wider(names_from = Y, values_from = n) %>%
    replace_na(list(`0` = 0, `1` = 0)) %>% 
    mutate(`Mean(Y)` = round(`1` / (`1` + `0`), 4)))
```
```{r}
(default_occurrence <- 
 ggplot(don_desbois %>% mutate(Y = if_else(Y == "1", 1, 0)), aes(x=r17, y=Y)) +
 geom_jitter(alpha=0.2, height=0.1) +
 geom_segment(data = don_desbois_binned,
              aes(x = min, xend = max, y = `Mean(Y)`, yend = `Mean(Y)`),
              color = 'coral',
              linewidth=1.25))
```

* perform train/test split and evaluate ROC / AUC for some `glm()` models of your choice (ie choose manually a subset of variables)

Simple random sampling (70/30)

```{r}
#| code-fold: show
# Using base R
set.seed(1987)  # for reproducibility
index_1 <- sample(1:nrow(don_desbois), round(nrow(don_desbois) * 0.7))
train_1 <- don_desbois[index_1, ]
test_1  <- don_desbois[-index_1, ]

# table(don_desbois$Y) %>% prop.table()
#        0        1 
# 0.518254 0.481746

# table(train_1$Y) %>% prop.table()
#         0         1 
# 0.5136054 0.4863946 

# table(test_1$Y) %>% prop.table()
#         0         1 
# 0.5291005 0.4708995 

# Using rsample package
set.seed(1987)  # for reproducibility
split_1  <- rsample::initial_split(don_desbois, prop = 0.7)
train_2  <- rsample::training(split_1)
test_2   <- rsample::testing(split_1)


```

Here the data set is balanced and train/test using a simple random sampling have a similar distribution. 

But in case of imbalance or if we want to control the split we may want to use Stratified sampling:

```{r}
#| code-fold: show
split_strat  <- rsample::initial_split(don_desbois, prop = 0.7, 
                              strata = "Y")
train_strat  <- rsample::training(split_strat)
test_strat   <- rsample::testing(split_strat)

# table(don_desbois$Y) %>% prop.table()
#        0        1 
# 0.518254 0.481746

# table(train_strat$Y) %>% prop.table()
#         0         1 
# 0.5187287 0.4812713  

# table(test_1$Y) %>% prop.table()
#         0         1 
# 0.5171504 0.4828496 

```

```{r}
#| code-fold: show
# We use to illustrate the model proposed in the article

#fit logistic regression model
model_desbois <- glm(Y ~ STATUS + HECTARE + r1 + r3 + r17 + r24 + r28 + r36,
                     data = train_strat,
                     family = "binomial")

# plot ROC / compute AUC for the training set
pred_train <- prediction(model_desbois$fitted.values, train_strat$Y)
perf_train <- performance(pred_train, measure = "tpr", x.measure = "fpr")
plot(perf_train, main="ROC curve Admissions", xlab="Specificity",
     ylab="Sensitivity", col = "darkorange")
abline(0, 1) #add a 45 degree line

auc_train <- performance(pred_train, measure = "auc")
auc_train <- auc_train@y.values[[1]]
# auc_train
# 0.9635595

# use fitted model to predict value on testing set
test_predict <- predict(model_desbois, newdata=test_strat, type="response")

# plot ROC / compute AUC for the training set
pred_test <- prediction(test_predict, test_strat$Y)
perf_test <- performance(pred_test, measure = "tpr", x.measure = "fpr")
plot(perf_test, add = TRUE, main="ROC curve Admissions", xlab="Specificity",
     ylab="Sensitivity", col = "dodgerblue")

auc_test <- performance(pred_test, measure = "auc")
auc_test <- auc_test@y.values[[1]]

```


* perform k-fold cross validation and for each fold evaluate ROC / AUC for the same models as before


Leveraging `rsample`, the `tidyverse` and stackoverflow:

```{r, warning=FALSE, message=FALSE}
#| code-fold: show
set.seed(1987)
folds_10  <- rsample::vfold_cv(train_strat, v = 10)

cvfun <- function(split, ...){
  mod <- glm(Y ~ STATUS + HECTARE + r1 + r3 + r17 + r24 + r28 + r36,
             data=rsample::analysis(split),
             family=binomial)
  fit <- predict(mod, newdata=rsample::assessment(split), type="response")
  data.frame(fit = fit, y = model.response(model.frame(formula(mod), data=rsample::assessment(split))))
}

cv_out <- folds_10 %>% 
    mutate(fit = purrr::map(splits, cvfun)) %>% 
    unnest(fit) %>% 
    group_by(id) %>% 
    summarise(auc = pROC::roc(y, fit, plot=FALSE)$auc[1])


```


```{r, warning=FALSE, message=FALSE, warning = FALSE}
#| code-fold: show
# https://stackoverflow.com/questions/66000977/roc-with-cross-validation-for-linear-regression-in-r
cv_out_plot <- folds_10 %>% 
  mutate(fit = map(splits, cvfun)) %>% 
  unnest(fit) %>% 
  group_by(id) %>% 
  summarise(sens = pROC::roc(y, fit, plot=FALSE)$sensitivities, 
              spec = pROC::roc(y, fit, plot=FALSE)$specificities, 
              obs = 1:length(sens))

ave <- cv_out_plot %>% 
  ungroup %>% 
  group_by(obs) %>% 
  summarise(sens = mean(sens), 
            spec = mean(spec), 
            id = "Average")

cv_out_plot <- bind_rows(cv_out_plot, ave) %>% 
  mutate(col = factor(ifelse(id == "Average", "Average", "Individual"), 
                      levels=c("Individual", "Average")))

ggplot(cv_out_plot , aes(x=1-sens, y=spec, group=id, colour=col)) + 
  geom_line(aes(size=col, alpha=col)) + 
  scale_colour_manual(values=c("black", "red")) + 
  scale_size_manual(values=c(.5,1.25)) + 
  scale_alpha_manual(values=c(.3, 1)) + 
  theme_classic() + 
  theme(legend.position=c(.75, .15)) + 
  labs(x="1-Sensitivity", y="Specificity", colour="", alpha="", size="")
```

k-fold cross validation manually

```{r}
#| code-fold: show
# set.seed(1987)
# 
# res_list <- list()
# nb_iter <- 2 # Usually 5 x 10 fold validation
# 
# for (j in 1:5) {
#     nb_blocks <- 10 # number of folds/blocks
#     blocks <- sample(rep(1:nb_blocks,nrow(don_desbois))[1:nrow(don_desbois)])
# 
#     result <- data.frame(matrix(nrow=dim(don_desbois),ncol=10))
# 
#     for (i in 1:nb_blocks) {
#           print(i)
# 
#           XX_train <- don_desbois[blocks!=i,]
#           XX_test <- don_desbois[blocks==i,]
# 
#           # glm full
#           class1 <- glm(Y ~ .,
#                         data=XX_train,
#                         family=binomial)
#           # glm desbois
#           class2 <- glm(Y ~ r1 + r32 + r14 + r17 + r36 + r12,
#                         data=XX_train,
#                         family=binomial)
# 
#           # stepwise methods
#           intercept_only <- glm(Y ~ 1, data=XX_train, family="binomial")
#           # forward aic
#           class3 <- step(intercept_only, direction='forward', test = 'LRT', scope=formula(class1),
#                          k=2, trace = FALSE)
#           # forward bic
#           class4 <- step(intercept_only, direction='forward', test = 'LRT', scope=formula(class1),
#                          k=log(nrow(don_desbois)), trace = FALSE)
#           # backward aic
#           class5 <- step(class1, direction='backward', test = 'LRT', scope=formula(class1),
#                          k=2, trace = FALSE)
#           # backward bic
#           class6 <- step(class1, direction='backward', test = 'LRT', scope=formula(class1),
#                          k=log(nrow(don_desbois)), trace = FALSE)
# 
# 
#           # penalized regression
#           # ridge
#           # class7 <- cv.glmnet(as.matrix(XX_train[,-which(names(XX_train)=="maxO3")]),XX_train$maxO3,alpha=0)
#           class7  <- glmnetUtils::cv.glmnet(Y ~ ., data=XX_train, family="binomial", alpha=0, type.measure = "auc")
#           # lasso
#           # class8 <- cv.glmnet(as.matrix(XX_train[,-which(names(XX_train)=="maxO3")]),XX_train$maxO3,alpha=1)
#           class8  <- glmnetUtils::cv.glmnet(Y ~ ., data=XX_train, family="binomial", alpha=1, type.measure = "auc")
#           # elastic-net
#           # class9 <- cv.glmnet(as.matrix(XX_train[,-which(names(XX_train)=="maxO3")]),XX_train$maxO3,alpha=0.5)
#           class9  <- glmnetUtils::cv.glmnet(Y ~ ., data=XX_train, family="binomial", alpha=0.5, type.measure = "auc")
# 
#           # cart
#           class10 <- rpart::rpart(Y~., data = XX_train, method = "class")
# 
#           # saving predictions on test set
#           result[blocks==i,1] = predict(class1, newdata=XX_test, type = "response")
#           result[blocks==i,2] = predict(class2, newdata=XX_test, type = "response")
#           result[blocks==i,3] = predict(class3, newdata=XX_test, type = "response")
#           result[blocks==i,4] = predict(class4, newdata=XX_test, type = "response")
#           result[blocks==i,5] = predict(class5, newdata=XX_test, type = "response")
#           result[blocks==i,6] = predict(class6, newdata=XX_test, type = "response")
#           # penalized - glmnet
#           result[blocks==i,7] = as.vector(predict(class7, newdata=XX_test, type = "response"))
#           result[blocks==i,8] = as.vector(predict(class8, newdata=XX_test, type = "response"))
#           result[blocks==i,9] = as.vector(predict(class9, newdata=XX_test, type = "response"))
#           # cart
#           result[blocks==i,10] = predict(class10, newdata=XX_test, type = "prob")[, 2]
# 
#     }
#     names(result) <- c("glm full",
#                        "glm desbois",
#                        "forward aic",
#                        "forward bic",
#                        "backward aic",
#                        "backward bic",
#                        "ridge",
#                        "lasso",
#                        "elastic-net",
#                        "cart")
# 
#     # gbm / svm lineaire / svm radial
#     erreur <- function(X,Y) { mean((X-Y)^2)}
# 
#     auc <- function(X,Y){
#            pred <- ROCR::prediction(X, Y)
#            auc <- ROCR::performance(pred, measure = "auc")
#            auc <- auc@y.values[[1]]
#     }
# 
#     res_list[[j]] = list(auc = apply(result, 2, auc, Y=don_desbois$Y), result = result)
# }
# saveRDS(res_list, "res_list.rds")
```


```{r}
res_list <- readRDS("res_list.rds")
res_list[[1]]$auc
res_list[[2]]$auc
```


# References

::: {#refs}
:::
