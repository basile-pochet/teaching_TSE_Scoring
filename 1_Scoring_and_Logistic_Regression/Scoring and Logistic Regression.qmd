---
title: "Scoring and Logistic Regression"
author: "Louis OLIVE"
bibliography: references.bib
link-citations: true
format:
  html:
    #theme: darkly
    highlight: espresso
    code-copy: true
    code-fold: true
    df-print: paged
    include-in-header: mathjax.html
    number-sections: true
    toc: true
    toc_depth: 3
    toc_float: yes
execute: 
  cache: true
  warning: false
editor: visual
fontsize: 11pt
---

```{r, echo = FALSE, warning = FALSE, message = FALSE}
library(tidyverse)
```

# Introduction and vocabulary

## A reminder (or not) on Supervised Learning

This an informal introduction on Statistical Learning, where we introduce concepts and definitions. For a rigorous introduction see for example the dedicated lesson from [Sébastien Gadat - D3S M1 course on Mathematical Statistics](https://perso.math.univ-toulouse.fr/gadat/files/2012/12/Intro_Stat_Learning.pdf) or [Francis Bach - Learning Theory from First Principles](https://www.di.ens.fr/%7Efbach/ltfp_book.pdf).

### Inputs-Outputs

We follow here the terminology of @hastie2009. The problem: we are given a data set where some variables, denoted as **inputs**[^1], have some influence on one or more **output(s)**[^2].

[^1]: In statistics, inputs are often called the predictors, factors or the independent variables. In machine learning the term features is also used.

[^2]: The outputs are also called responses, labels or dependent variables.

We will denote inputs by the symbol $X$ having values in $\mathrm{X}$ (to explicit things we will consider $\mathrm X$ is $\mathbb R^p$). If $X$ is a vector, its components can be accessed by subscripts $X_j$, $j = 1, . . . , p$.

Outputs will be denoted by $Y$ having value in $\mathrm Y$:

### Supervised learning framework

-   When $\mathrm Y$ is $\mathbb R$, it is a **regression problem**;

-   When $\mathrm Y$ is a discrete set, it is a **classification problem** (for example $\mathrm Y$ is $\{0,1\}$ for binary classification).

In this course we will restrict to the case of **binary classification**.

We need data to construct predictions or classifications. We suppose we have available a set of observed data: $(x_i, y_i) \in \mathrm X \times \mathrm Y$ , $i = 1, \cdots ,n$, known as the **learning** or **training** set, with which to construct our prediction.

We assume $(x_i, y_i) \in \mathrm X \times \mathrm Y$ are generated i.i.d. from:

$$
\mathbf P_{\mathrm X \times \mathrm Y}
$$

where we denote $\mathbf P_{\mathrm X \times \mathrm Y}$, the data generating distribution (or underlying probability distribution or joint law) which is unknown.

The main goal of supervised learning will be to predict unobserved outputs $y$ given unobserved inputs $x$ using the learning set. Such new or unobserved data is referred as the **testing** set.

### Loss and Risk

To achieve that goal we try to find a "best" or optimal **classifier** (or prediction function in general):

$$
f: \mathrm X \to \mathrm Y
$$

We first must define some criterion to assess the classifier performance (what is optimal?).

For that purpose we consider a **loss** function $\ell: \mathrm Y \times \mathrm Y \to \mathbb R^+$:

$$
\left\{ \begin{array}{ll} 
    \ell(y,z) > 0 &  \mbox{if } y \neq z\cr
    \ell(y,z) = 0 &  \mbox{if } y = z\cr
\end{array} \right.
$$

$\ell(y,f(x))$ is the loss or cost of predicting $f(x)$ while the true output is $y$.

In the context of binary classification a natural selection for loss is the 0-1 loss:

$$
\ell(y,z)=\mathbf 1_{y \neq z}
$$

We will see later in the course that other losses are used.

We then define the expected **risk** of a prediction function $f$ given the loss $\ell$ as:

$$
\mathrm R(f) = \mathbb{E}[\ell(Y,f(X))]
$$ In the context of binary classification $Y \in \{0,1\}$ and 0-1 loss, we have:

$$
\mathrm R(f) = \mathbb{P}[Y\neq f(X))]
$$

Given a data set $(x_i, y_i) \in \mathrm X \times \mathrm Y$, we also define the empirical risk of a classifier $f$ as:

$$
\hat{\mathrm R}(f) = \frac{1}{n} \sum_i \ell(y_i,f(x_i))
$$

### Bayes classifier

We now define the **Bayes(ian) classifier** $f^*: \mathrm X \to \mathrm Y$ as a function that achieves the minimal expected risk among all possible functions:

$$
\underset{f}{\operatorname{argmin}}\mathrm R(f) 
$$

It is sometimes designed as oracle in the literature.

The Bayes classifier depends on $\ell$ and $\mathbf P_{\mathrm X \times \mathrm Y}$ which is generally unknown (otherwise the job would be easy).

The Bayes classifier for the 0-1 loss is:

$$
f^*(x)=\left\{ \begin{array}{ll} 
    1 &  \mbox{if } \eta(x)\geq \frac{1}{2}\cr
    0 &  \mbox{otherwise}\cr
\end{array} \right.
$$

where:

$$
\eta(x)=\mathbb{P}[Y=1|X=x)] = \mathbb{E}[Y=1|X=x)]
$$

$\eta(x)$ is called the regression function.

### Theory vs practice

In practice $\mathbf P_{\mathrm X \times \mathrm Y}$ is unknown. What we have is the learning set. What to do?

Conceptually two approaches are used[^3]:

[^3]: Here I use Sébastien Gadat terminology which is very clear;

-   the "generative" or "probabilistic" approach: we model the join distribution $\mathbf P_{\mathrm X \times \mathrm Y}$, we estimate the regression function $\eta(x)$ and use this estimate "inside" the Bayes classifier;

-   the optimization or "machine learning" approach: minimize the empirical risk; usually re-sampling methods are used to compute empirical risk (for example $f$ is trained on a training set and empirical risk evaluated on the testing set) and optimization algorithm or heuristics to estimate $f$.

We will see later that in the context of the Logistic Regression the two approaches are strongly linked.

## Illustrative example: the **Mixture** data set

To illustrate what we have seen we use the **Mixture** data set described in @hastie2009 [p. 12-16].

The data set is generated as follows (p. 16):

-   First the authors generate 10 "means" $m_k$ in $\mathbb R^2$ from a bivariate Gaussian distribution $\mathcal{N}((1,0),I)$ and label this class *BLUE*.

-   Similarly, 10 more are drawn from $\mathcal{N}((0,1),I)$ and labeled *ORANGE*.

-   Then for each class authors generate 100 observations as follows: for each observation, $m_k$ is picked at random with probability 1/10 and used to generate a $\mathcal{N}(m_k,I/5)$, thus leading to a mixture of Gaussian clusters for each class. They generate similarly an additional data set of 5k observations per class for testing purposes.

The task is to create a classifier that, based on the coordinates $x_1$ and $x_2$, determines whether the point is *ORANGE* or *BLUE*.

First we load the data set and plot the "means":

```{r}
# Simulated mixture (ORANGE/BLUE) from ESLII/ISLR
load(file='../data/mixture.example.RData')

x1_means <- mixture.example$means[,1]
x2_means <- mixture.example$means[,2]
mixture_means <- tibble(x1_means, x2_means) %>%
    rowid_to_column() %>%
    mutate(Y = if_else(rowid <= 10, "BLUE", "ORANGE"))

ggplot(mixture_means) + 
    geom_point(aes(x = x1_means, y = x2_means, col = Y), size = 3, show.legend = FALSE) +
    scale_colour_manual(values = c("dodgerblue", "orange")) +
    theme_void()

# saveRDS(mixture_means, "mixture_means.rds")
```

Then the training set together with the "means":

```{r}
Y = mixture.example$y
x1 = mixture.example$x[,1]
x2 = mixture.example$x[,2]
data_mixture_example <- tibble(Y, x1, x2) %>% mutate(Y = as_factor(Y))

ggplot(data_mixture_example) + 
    geom_point(aes(x = x1, y = x2, col = Y), shape = "o", size = 4, stroke = 2, show.legend = FALSE) +
    geom_point(data = mixture_means, aes(x = x1_means, y = x2_means, col =Y), size = 3, show.legend = FALSE) +
    scale_colour_manual(values = c("dodgerblue", "orange", "dodgerblue", "orange")) +
    theme_void()

#saveRDS(data_mixture_example, "data_mixture_example.rds")

```

Knowing the generative distribution, we generate a testing set of size 10k (half BLUE, half ORANGE).

```{r}
# generate new sampling
set.seed(1987)
N <- 10000
draw <- sample(1:10, size=N, replace=TRUE)
x1_blue <- rnorm(N/2) * 1/sqrt(5) + mixture_means[draw[1:(N/2)],2]
x2_blue <- rnorm(N/2) * 1/sqrt(5) + mixture_means[draw[1:(N/2)],3]
x1_orange <- rnorm(N/2) * 1/sqrt(5) + mixture_means[10 + draw[(N/2+1):N],2]
x2_orange <- rnorm(N/2) * 1/sqrt(5) + mixture_means[10 + draw[(N/2+1):N],3]

new_mixture <- rbind(cbind(x1_blue, x2_blue, Y="BLUE"), cbind(x1_orange, x2_orange, Y="ORANGE")) %>%
    as_tibble() %>% 
    rename(x1=x1_means, x2=x2_means)

# head(new_mixture)
```

We plot the first 250 new observations of each class from the generated testing set:

```{r}
ggplot(new_mixture[c(1:250,5001:5251),]) + 
    geom_point(aes(x = x1, y = x2, col = Y), shape = "o", size = 4, stroke = 2, show.legend = FALSE) +
    geom_point(data = mixture_means, aes(x = x1_means, y = x2_means, col =Y), size = 3, show.legend = FALSE) +
    scale_colour_manual(values = c("dodgerblue", "orange", "dodgerblue", "orange")) +
    theme_void()
# saveRDS(new_mixture, "new_mixture.rds")
# new_mixture <- readRDS("new_mixture.rds")
```

### Bayes classifier

Knowing the generating distribution we can derive the Bayes classifier (exercise 2.2 [@hastie2009]).

We plot the boundary decision in grey:

```{r}
grid <- expand.grid(x1 = seq(-2.6, 4.2, .05), x2 = seq(-2.0, 2.9, .05)) %>% as_tibble()

# Prediction function used to classify areas on the grid and imply the decision boundary
predict_oracle <- function(x1, x2){
    obj <- 0
    for(i in 1:10){
       obj <- obj +
           exp(-5/2*((x1-x1_means[i])**2+(x2-x2_means[i])**2)) -
           exp(-5/2*((x1-x1_means[i+10])**2+(x2-x2_means[i+10])**2))
    }
    1 * (obj < 0)
}

predict_oracle_V <- Vectorize(predict_oracle)

grid <- grid %>% mutate(predict_oracle = predict_oracle_V(x1, x2))

ggplot(grid) + 
geom_point(aes(x = x1, y = x2, col = as.factor(predict_oracle)),
           shape = 20, size = .05, alpha = .5, show.legend = FALSE) +
geom_contour(aes(x = x1, y = x2, z = predict_oracle),
             breaks = 0.05, col = 'darkgrey') + 
geom_point(data = data_mixture_example, aes(x = x1, y = x2, col = Y),
           shape = "o", size = 4, stroke = 2, show.legend = FALSE) +
scale_colour_manual(values = c("dodgerblue", "orange")) +
theme_void()
```

We estimate Bayes risk on the testing data set simulated before (10k BLUE/ORANGE dots).

```{r}
bayes_error_rate <- new_mixture %>%
    mutate(Y = if_else(Y=="BLUE", 0, 1),
           predict_oracle = predict_oracle_V(x1, x2),
           l01 = if_else(Y==predict_oracle, 0, 1))

bayes_test_risk <- bayes_error_rate %>%
    summarise(l01 = mean(l01)) %>% 
        pull(l01)

bayes_error_rate_train <- data_mixture_example %>%
    mutate(Y = if_else(Y=="0", 0, 1),
           predict_oracle = predict_oracle_V(x1, x2),
           l01 = if_else(Y==predict_oracle, 0, 1))

bayes_train_risk <- bayes_error_rate_train %>%
    summarise(l01 = mean(l01)) %>% 
        pull(l01)
```

We get a value of `r round(bayes_test_risk,3)` for the Bayes risk (in line with [@hastie2009 Figure 2.4 Misclassification curves]).

We give below a brief tour showing how common classifiers perform.

### Logistic Regression classifier

We plot below the linear decision boundary given by a Logistic Regression vs Bayes:

```{r}
grid <- expand.grid(x1 = seq(-2.6, 4.2, .05), x2 = seq(-2.0, 2.9, .05)) %>% as_tibble()

mixture_example_logit <- glm(Y~., data = data_mixture_example, family = "binomial")

grid <- broom::augment(mixture_example_logit, data = data_mixture_example,
               newdata = grid, type.predict = "response", type.residuals = "deviance") %>% 
    mutate(predict_glm = 1*(.fitted >= 0.5),
           predict_oracle = predict_oracle_V(x1, x2))

coeffs <- mixture_example_logit$coefficients

glm_boundary <- function(x1){
    -(coeffs[2] * x1 + coeffs[1]) / coeffs[3]
}
glm_boundary_V <- Vectorize(glm_boundary) 

ggplot(grid) + 
geom_point(aes(x = x1, y = x2, col = as.factor(predict_glm)),
            shape = 20, size = .05, alpha = .5, show.legend = FALSE) +
geom_contour(aes(x = x1, y = x2, z = predict_oracle),
             breaks = 0.05, col = 'purple') + 
geom_line(aes(x = x1, y = glm_boundary(x1)),col = 'darkgrey') +
geom_point(data = data_mixture_example, aes(x = x1, y = x2, col = Y),
           shape = "o", size = 4, stroke = 2, show.legend = FALSE) +
scale_colour_manual(values = c("dodgerblue", "orange")) +
theme_void()
```

We add a sample of the testing set to the preceding graph:

```{r}
grid <- broom::augment(mixture_example_logit, 
                       data = data_mixture_example,
                       newdata = grid,
                       type.predict = "response",
                       type.residuals = "deviance") %>% 
    mutate(predict_glm = 1*(.fitted >= 0.5),
           predict_oracle = predict_oracle_V(x1, x2))

coeffs <- mixture_example_logit$coefficients

glm_boundary <- function(x1){
    -(coeffs[2] * x1 + coeffs[1]) / coeffs[3]
}
glm_boundary_V <- Vectorize(glm_boundary) 

ggplot(grid) + 
geom_point(aes(x = x1, y = x2, col = as.factor(predict_glm)),
            shape = 20, size = .05, alpha = .5, show.legend = FALSE) +
geom_contour(aes(x = x1, y = x2, z = predict_oracle),
             breaks = 0.05, col = 'purple') + 
geom_line(aes(x = x1, y = glm_boundary(x1)),col = 'darkgrey') +
geom_point(data = data_mixture_example, aes(x = x1, y = x2, col = Y),
           shape = "o", size = 4, stroke = 2, show.legend = FALSE) +
geom_point(data = new_mixture[c(1:500,5001:5501),], aes(x = x1, y = x2, col = Y),
           shape = "o", size = 4, stroke = 2, show.legend = FALSE) +
scale_colour_manual(values = c("dodgerblue", "orange", "dodgerblue", "orange")) +
theme_void()
```

```{r}
logit_error_rate <- broom::augment(mixture_example_logit,
                                   data = data_mixture_example,
                                   newdata = new_mixture,
                                   type.predict = "response",
                                   type.residuals = "deviance") %>% 
                    mutate(Y = if_else(Y == "BLUE", 0, 1),
                           predict_glm = 1*(.fitted >= 0.5),
                           l01 = if_else(Y==predict_glm, 0, 1))

logit_risk <- logit_error_rate %>% summarise(l01 = mean(l01)) %>% pull(l01)
```

The empirical risk on testing set is `r round(logit_risk, 3)`.

### Nonparametric (splines) Logistic Regression classifier

A popular method to move beyond linearity is to transform variables, for example using splines [@hastie2009 5.6 Nonparametric Logistic Regression, p161-164].

This can achieve more flexibility at the decision boundary.

Using natural splines in $x_1$ and $x2$:

```{r}
library(splines)

## fit additive natural cubic spline model
# Additive Natural Cubic Splines - 4 df each (Fig 5.11 p164)
grid <- expand.grid(x1 = seq(-2.6, 4.2, .05), x2 = seq(-2.0, 2.9, .05)) %>% as_tibble()

mixture_example_glm_splines_add <- glm(Y ~ splines::ns(x1, df=4) + 
                                           splines::ns(x2, df=4),
                                       data = data_mixture_example,
                                       family = "binomial")

grid <- broom::augment(mixture_example_glm_splines_add,
                       data = data_mixture_example,
                       newdata = grid,
                       type.predict = "response",
                       type.residuals = "deviance") %>% 
               mutate(predict_gam = 1*(.fitted >= 0.5),
                      predict_oracle = predict_oracle_V(x1, x2))

ggplot(grid) + 
geom_point(aes(x = x1, y = x2, col = as.factor(predict_gam)),
            shape = 20, size = .05, alpha = .5, show.legend = FALSE) +
geom_contour(aes(x = x1, y = x2, z = predict_oracle),
             breaks = 0.05, col = 'purple') + 
geom_contour(aes(x = x1, y = x2, z = predict_gam),
             breaks = 0.05, col = 'darkgrey') + 
geom_point(data = data_mixture_example, aes(x = x1, y = x2, col = Y),
           shape = "o", size = 4, stroke = 2, show.legend = FALSE) +
scale_colour_manual(values = c("dodgerblue", "orange")) +
theme_void()
```

```{r}
logit_splines_add_error_rate <- broom::augment(mixture_example_glm_splines_add,
                                   data = data_mixture_example,
                                   newdata = new_mixture,
                                   type.predict = "response",
                                   type.residuals = "deviance") %>% 
                                mutate(Y = if_else(Y == "BLUE", 0, 1),
                                       predict_glm = 1*(.fitted >= 0.5),
                                       l01 = if_else(Y==predict_glm, 0, 1))

logit_splines_add_risk <- logit_splines_add_error_rate  %>%
    summarise(l01 = mean(l01)) %>% 
    pull(l01)
```

The empirical risk on testing set is `r round(logit_splines_add_risk, 3)`.

Using the interactions of splines in $x_1$ and $x2$:

```{r}

## fit additive natural cubic spline model
# Natural Cubic Splines - Tensor Product - 4 df each (Fig 5.11 p164)
grid <- expand.grid(x1 = seq(-2.6, 4.2, .05), x2 = seq(-2.0, 2.9, .05)) %>% as_tibble()

mixture_example_glm_splines_add_tens <- glm(Y ~ splines::ns(x1, df=4) : # interaction x1/x2
                                                splines::ns(x2, df=4)-1,
                                            data = data_mixture_example,
                                            family = "binomial")

grid <- broom::augment(mixture_example_glm_splines_add_tens,
                       data = data_mixture_example,
                       newdata = grid,
                       type.predict = "response",
                       type.residuals = "deviance") %>% 
               mutate(predict_gam = 1*(.fitted >= 0.5),
                      predict_oracle = predict_oracle_V(x1, x2))

ggplot(grid) + 
geom_point(aes(x = x1, y = x2, col = as.factor(predict_gam)),
           shape = 20, size = .05, alpha = .5, show.legend = FALSE) +
geom_contour(aes(x = x1, y = x2, z = predict_oracle),
             breaks = 0.05, col = 'purple') + 
geom_contour(aes(x = x1, y = x2, z = predict_gam),
             breaks = 0.05, col = 'darkgrey') + 
geom_point(data = data_mixture_example, aes(x = x1, y = x2, col = Y),
           shape = "o", size = 4, stroke = 2, show.legend = FALSE) +
scale_colour_manual(values = c("dodgerblue", "orange")) +
theme_void()

```

```{r}

## fit additive natural cubic spline model
# Natural Cubic Splines - Tensor Product - 4 df each (Fig 5.11 p164)
grid <- expand.grid(x1 = seq(-2.6, 4.2, .05), x2 = seq(-2.0, 2.9, .05)) %>% as_tibble()

mixture_example_glm_splines_add_tens <- glm(Y ~ splines::ns(x1, df=4) : # interaction x1/x2
                                                splines::ns(x2, df=4)-1,
                                            data = data_mixture_example,
                                            family = "binomial")

grid <- broom::augment(mixture_example_glm_splines_add_tens,
                       data = data_mixture_example,
                       newdata = grid,
                       type.predict = "response",
                       type.residuals = "deviance") %>% 
               mutate(predict_gam = 1*(.fitted >= 0.5),
                      predict_oracle = predict_oracle_V(x1, x2))

ggplot(grid) + 
geom_point(aes(x = x1, y = x2, col = as.factor(predict_gam)),
           shape = 20, size = .05, alpha = .5, show.legend = FALSE) +
geom_contour(aes(x = x1, y = x2, z = predict_oracle),
             breaks = 0.05, col = 'purple') + 
geom_contour(aes(x = x1, y = x2, z = predict_gam),
             breaks = 0.05, col = 'darkgrey') + 
geom_point(data = data_mixture_example, aes(x = x1, y = x2, col = Y),
           shape = "o", size = 4, stroke = 2, show.legend = FALSE) +
geom_point(data = new_mixture[c(1:500,5001:5501),], aes(x = x1, y = x2, col = Y),
           shape = "o", size = 4, stroke = 2, show.legend = FALSE) +
scale_colour_manual(values = c("dodgerblue", "orange", "dodgerblue", "orange")) +
theme_void()

```

```{r}
logit_splines_add_tens_error_rate <- broom::augment(mixture_example_glm_splines_add_tens,
                                   data = data_mixture_example,
                                   newdata = new_mixture,
                                   type.predict = "response",
                                   type.residuals = "deviance") %>% 
                                   mutate(Y = if_else(Y == "BLUE", 0, 1),
                                          predict_glm = 1*(.fitted >= 0.5),
                                          l01 = if_else(Y==predict_glm, 0, 1))

logit_splines_add_tens_risk <- logit_splines_add_tens_error_rate  %>%
    summarise(l01 = mean(l01)) %>% 
    pull(l01)

```

The empirical risk on testing set is `r round(logit_splines_add_tens_risk, 3)`.

### Logistic regression with binning:

It is usual, for example in the context of banking, to discretize or categorize numerical variables (a.k.a binning).

Here we use a simple decile binning on $x_1$ and $x_2$

```{r}
grid <- expand.grid(x1 = seq(-2.6, 4.2, .05), x2 = seq(-2.0, 2.9, .05)) %>% as_tibble()

breaks_x1 <- c(quantile(data_mixture_example$x1, probs = seq(0, 1, by = 1/10), na.rm = T))
breaks_x1[1] <- -1e8
breaks_x1[length(breaks_x1)] <- 1e8

breaks_x2 <- c(quantile(data_mixture_example$x2, probs = seq(0, 1, by = 1/10), na.rm = T))
breaks_x2[1] <- -1e8
breaks_x2[length(breaks_x2)] <- 1e8

data_binned <- data_mixture_example %>% 
    mutate(x1_bin = cut(x1, breaks = breaks_x1,
                    right = FALSE,
                    dig.lab = 1,
                    ordered_result = TRUE),
           x2_bin = cut(x2, breaks = breaks_x2,
                    right = FALSE,
                    dig.lab = 1,
                    ordered_result = TRUE))

grid <- grid %>% 
    mutate(x1_bin = cut(x1, breaks = breaks_x1,
                    right = FALSE,
                    dig.lab = 1,
                    ordered_result = TRUE),
           x2_bin = cut(x2, breaks = breaks_x2,
                    right = FALSE,
                    dig.lab = 1,
                    ordered_result = TRUE))

# from https://search.r-project.org/R/refmans/base/html/cut.html
# x1_labs <- levels(data_binned$x1_bin)
# x2_labs <- levels(data_binned$x2_bin)

#(,] 
# cbind(lower = as.numeric( sub("\\((.+),.*", "\\1", x1_labs) ),
#       upper = as.numeric( sub("[^,]*,([^]]*)\\]", "\\1", x1_labs) ))

#[,)
# x1_bounds <- bind_cols(lower = as.numeric( sub("\\[(.+),.*", "\\1", x1_labs) ),
                # upper = as.numeric( sub("[^,]*,([^\\)]*)(\\)|])", "\\1", x1_labs) ))

mixture_example_glm_binned <- glm(Y ~ x1_bin + x2_bin,
                                            data = data_binned,
                                            family = "binomial")

grid <- broom::augment(mixture_example_glm_binned,
                       data = data_binned,
                       newdata = grid,
                       type.predict = "response",
                       type.residuals = "deviance") %>% 
               mutate(predict_glm_binned = 1*(.fitted >= 0.5),
                      predict_oracle = predict_oracle_V(x1, x2))

ggplot(grid) + 
geom_point(aes(x = x1, y = x2, col = as.factor(predict_glm_binned)),
           shape = 20, size = .05, alpha = .5, show.legend = FALSE) +
geom_contour(aes(x = x1, y = x2, z = predict_oracle),
             breaks = 0.05, col = 'purple') + 
geom_contour(aes(x = x1, y = x2, z = predict_glm_binned),
             breaks = 0.05, col = 'darkgrey') + 
geom_point(data = data_mixture_example, aes(x = x1, y = x2, col = Y),
           shape = "o", size = 4, stroke = 2, show.legend = FALSE) +
scale_colour_manual(values = c("dodgerblue", "orange")) +
theme_void()


```

```{r}
new_mixture_binned <- new_mixture %>% 
    mutate(x1_bin = cut(x1, breaks = breaks_x1,
                    right = FALSE,
                    dig.lab = 1,
                    ordered_result = TRUE),
           x2_bin = cut(x2, breaks = breaks_x2,
                    right = FALSE,
                    dig.lab = 1,
                    ordered_result = TRUE))


logit_binned_error_rate <- broom::augment(mixture_example_glm_binned,
                                   data = data_binned,
                                   newdata = new_mixture_binned,
                                   type.predict = "response",
                                   type.residuals = "deviance") %>% 
                                mutate(Y = if_else(Y == "BLUE", 0, 1),
                                       predict_glm = 1*(.fitted >= 0.5),
                                       l01 = if_else(Y==predict_glm, 0, 1))

logit_binned_risk <- logit_binned_error_rate  %>%
    summarise(l01 = mean(l01)) %>% 
    pull(l01)
```

The empirical risk on testing set is `r round(logit_binned_risk, 3)`.

```{r}
# Optionnal - plot the graph for logistic regression using  binning + "WOE/IV" feature engineering (TODO if shown in class)
```

### K-Nearest Neighbors Algorithm

k-NN is a non-parametric algorithm. Given a new observation, it finds in the training set the k closest points (given a certain distance) and predicts using a majority vote.

We show below the boundary decision for $k=15$:

```{r}
library(class)
k <- 15

grid <- expand.grid(x1 = seq(-2.6, 4.2, .05), x2 = seq(-2.0, 2.9, .05)) %>% as_tibble()

# Normalize using boundaries of training sample
normalize <- FALSE # TRUE (pred error 0.259) / FALSE (pred error 0.2498)

if (normalize) {
    
    min1 <- min(data_mixture_example$x1)
    min2 <- min(data_mixture_example$x2)
    max1 <- max(data_mixture_example$x1)
    max2 <- max(data_mixture_example$x2)
    
    train_mixture <- data_mixture_example %>% 
        mutate(x1 = (x1 - min1) / (max1 - min1),
               x2 = (x2 - min2) / (max2 - min2)) %>% 
        select(x1, x2)
    
    test_grid_mixture <- grid %>% 
        mutate(x1 = (x1 - min1) / (max1 - min1),
               x2 = (x2 - min2) / (max2 - min2)) %>% 
        select(x1, x2)
    
    test_new_mixture <- new_mixture %>% 
        mutate(x1 = (x1 - min1) / (max1 - min1),
               x2 = (x2 - min2) / (max2 - min2)) %>% 
        select(x1, x2)
    
} else {
    
    train_mixture <- data_mixture_example %>%
        select(x1,x2)
    
    test_grid_mixture <- grid %>%
          select(x1,x2)
    
    test_new_mixture <- new_mixture %>%
        select(x1,x2)
    
}

mixture_example_knn <- knn(train_mixture, #train X
                           test_new_mixture, # test X
                           data_mixture_example %>% pull(Y), # train Y
                           k)

table_test <- table(mixture_example_knn,
                     new_mixture %>% # test Y
                        mutate(Y = as_factor(if_else(Y == "BLUE", 0, 1))) %>%
                        pull(Y))



mixture_example_knn <- knn(train_mixture, #train X
                           train_mixture, # test X = train X
                           data_mixture_example %>% pull(Y), # train Y
                           k)


table_train <- table(mixture_example_knn,
      data_mixture_example %>% pull(Y))


mixture_example_knn <- knn(train_mixture, #train X
                           test_grid_mixture, # test X = "dotted" grid for plot
                           data_mixture_example %>% pull(Y), # train Y
                           k)
    

grid <- bind_cols(grid, predict_knn = mixture_example_knn) %>%
    mutate(predict_knn = if_else(mixture_example_knn=='0', 0, 1),
           predict_oracle = predict_oracle_V(x1, x2))

ggplot(grid) + 
geom_point(aes(x = x1, y = x2, col = as.factor(predict_knn)),
           shape = 20, size = .05, alpha = .5, show.legend = FALSE) +
geom_contour(aes(x = x1, y = x2, z = predict_oracle),
             breaks = 0.05, col = 'purple') + 
geom_contour(aes(x = x1, y = x2, z = predict_knn),
             breaks = 0.05, col = 'darkgrey') + 
geom_point(data = data_mixture_example, aes(x = x1, y = x2, col = Y),
           shape = "o", size = 4, stroke = 2, show.legend = FALSE) +
scale_colour_manual(values = c("dodgerblue", "orange")) +
theme_void()
```

```{r}
knn_error_rate <- (table_test[[2]] + table_test[[3]]) /
(table_test[[1]] + table_test[[2]] + table_test[[3]] + table_test[[4]])
```

The empirical risk on testing set is `r round(knn_error_rate, 3)`.

We vary $k$ and show the empirical risks on training and testing sets together with Bayes risk:

```{r}

K = seq(100,1,-1)

misclass_curve <- list()

for (k in K){
    
    knn_test <- knn(train_mixture, #train X
                    test_new_mixture, # test X
                    data_mixture_example %>% pull(Y), # train Y
                    k)
    
    knn_train <- knn(train_mixture, #train X
                     train_mixture, # test X = train X
                     data_mixture_example %>% pull(Y), # train Y
                     k)
    
    table_test <- table(knn_test,
                        new_mixture %>% # test Y
                        mutate(Y = as_factor(if_else(Y == "BLUE", 0, 1))) %>%
                        pull(Y))

    test_error <- (table_test[[2]] + table_test[[3]]) / 
        (table_test[[1]] + table_test[[2]] + table_test[[3]] + table_test[[4]])
    
    table_train <- table(knn_train,
          data_mixture_example %>% pull(Y))

    train_error <- (table_train[[2]] + table_train[[3]]) / 
        (table_train[[1]] + table_train[[2]] + table_train[[3]] + table_train[[4]])
    
    misclass_curve[[k]] <- tibble(`k - Number of Nearest Neighbours` = k,
                                  `KNN train` = train_error,
                                  `KNN test` = test_error)
    
}

misclass_plot <- bind_rows(misclass_curve) %>% 
    mutate(`Bayes test` = bayes_test_risk,
           `Bayes train` = bayes_train_risk) %>% 
    pivot_longer(-`k - Number of Nearest Neighbours`,
                 names_to = "Model data",
                 values_to = "Prediction Error")
```

```{r, message = FALSE}
library(scales)

ggplot(misclass_plot) +
geom_line(aes(x = `k - Number of Nearest Neighbours`,
              y = `Prediction Error`,
              col = as.factor(`Model data`))) +
    scale_x_continuous(
        trans = scales::compose_trans("log10", "reverse"),
        breaks = c(100, 50, 15, 3, 1)) +
scale_colour_manual(values = c("purple", "green", "orange", "dodgerblue")) +
theme_bw() +
labs(col = NULL)

```

### Decision Trees (CART)

To finish our tour, we show the result of a recursive partition of the plan using Decision Trees techniques:

```{r}
library(rpart)
library(rpart.plot) 

grid <- expand.grid(x1 = seq(-2.6, 4.2, .05), x2 = seq(-2.0, 2.9, .05)) %>% as_tibble()
mixture_example_CART <- rpart(Y~., data = data_mixture_example, method = "class")

grid <- bind_cols(grid,
                  as_tibble(predict(mixture_example_CART,newdata = grid))) %>%
                    select(-`0`) %>% 
                    rename(predict_CART = `1`) %>% 
                    mutate(predict_CART = 1*(predict_CART >= 0.5),
                    predict_oracle = predict_oracle_V(x1, x2))

rpart.plot(mixture_example_CART)
```

```{r}
ggplot(grid) + 
geom_point(aes(x = x1, y = x2, col = as.factor(predict_CART)),
               shape = 20, size = .05, alpha = .5, show.legend = FALSE) +
geom_contour(aes(x = x1, y = x2, z = predict_oracle),
             breaks = 0.05, col = 'purple') + 
geom_contour(aes(x = x1, y = x2, z = predict_CART),
             breaks = 0.05, col = 'darkgrey') + 
geom_point(data = data_mixture_example, aes(x = x1, y = x2, col = Y),
           shape = "o", size = 4, stroke = 2, show.legend = FALSE) +
scale_colour_manual(values = c("dodgerblue", "orange")) +
theme_void()
```

```{r}
cart_error_rate <- bind_cols(new_mixture,
                          tibble(class = predict(mixture_example_CART,
                                                 newdata = new_mixture,
                                                 type = "class")),
                          tibble(prob = predict(mixture_example_CART,
                                                 newdata = new_mixture,
                                                 type = "prob")[, 2])) %>%
                        #  as_tibble()) %>% 
                        mutate(Y = if_else(Y == "BLUE", 0, 1),
                               l01 = if_else(Y==class, 0, 1))
cart_risk <- cart_error_rate  %>%
    summarise(l01 = mean(l01)) %>% 
    pull(l01)
```

The empirical risk on testing set is `r round(cart_risk, 3)`.


## Scoring

At last we come to the subject of our course!

We remind we are in the context of binary classification.

Often we (and the business) are more interested in estimating the probabilities that $Y$ belongs to each class.

Quoting Trevor Hastie: "For example, it is more valuable to have an estimate of the probability that an insurance claim is fraudulent, than a classification fraudulent or not." This applies to many uses cases:

-   propensity score: is a customer interested by a product or service ? will the user click on the ad?

-   behavioral score: may a customer encounter a delinquency, a bankruptcy ?

-   application score: is the bank ready to grant a loan or a credit card to a customer to authorize a given transaction ?

-   churn score: will a newly acquired customer stay long enough for the account to be profitable ?

-   fraud score : is an application or a transaction fraudulent?

Scoring has business implications and is used as a decision tool for risk assessment or costs reduction:

-   improve the rate of return of marketing campaigns : reach more receptive customers without increasing the number of mailings or reach as many receptive customers with smaller mailings

-   improve efficiency of credit decisions : treat faster customers with low score and concentrate attention on customers with medium score

-   increase homogeneity of decisions across sales managers, and across applications

-   reduce the number of overdue

-   also used for pricing (adapt price to risk)

Before teaching this course I understood "Scoring" as applied classification with business perspective (i.e. Credit scoring for banks).

But we will see that this is more subtle than that, and that there is a precise definition of Scoring.

We follow in the rest of the section the terminology of @cornillon2019 [chapter 11.6 "Prévision - scoring"].

The aim of Scoring is to find a Scoring function or Score $S: \mathbb R^p \to \mathbb R$ that is "high" in case $\mathbb{P}[Y=1|X=x)$ is "high" and "low" in case $\mathbb{P}[Y=0|X=x)$ is "high".

We say that $S(x)$ is the score of the observation $x \in \mathbb R^p$.

We show below a simplified example of a credit scorecard that were typically used by banks to assess the creditworthiness of consumer loans applicants (as shown in [@scoringThomas]):

![](images/scoring_scorecard.png){fig-align="center" width="350"}

In this example a 35-year-old owner wishing to borrow money for home improvement and that has never had a county court judgement (CCJ) will score $129 = (36+25+36+32)$, while a 20-year-old living with his parents borrowing for holiday and having more than £1200 of CCJ will score $38=(22+14+19-17)$.

Some remarks:

-   the notions of Score and classifier are closely linked. Given a Score $S$ and a cutoff $s \in \mathbb R$ we obtain the following classifier:

$$
f_s(x)=\left\{ \begin{array}{ll} 
    1 &  \mbox{if } S(x)\geq s\cr
    0 &  \mbox{otherwise}\cr
\end{array} \right.
$$

-   The values taken by $S(x)$ are less important than the way they will order a set of observation $x_1,...x_n$. For any $\phi: \mathbb R \to \mathbb R$ bijective and increasing function, we say that the scores $S$ and $\phi \circ S$ are equivalent.

-   We defined above, in the context of supervised learning, the regression function $\eta(x)=\mathbb{P}[Y=1|X=x)]$. It is a natural candidate for a Scoring function. The related Bayes classifier uses $s=\frac{1}{2}$ as cutoff. As we have just seen in the last sections, $\eta(x)$ is unknown and we will have to approximate or estimate this regression function using the training set.

There are many ways to assess the performance of a Score.

We first define the **confusion matrix**:

|       | $f_s(X)=0$ | $f_s(X)=1$ |     |
|:------|:----------:|:----------:|:---:|
| $Y=0$ |     TN     |     FP     |  N  |
| $Y=1$ |     FN     |     TP     |  P  |

: {.bordered}

The confusion matrix is a contingency table which cross-tabulates $Y$ with the predicted one $f_s(X)$.

It can be evaluated on the learning set as well as on the testing set.

The following vocabulary originates from medical applications:

-   true positive (TP)

-   true negative (TN),

-   false positive (FP), also Type I error

-   false negative (FN), also Type II error

More formally we define for a given cutoff $s$:

$$
\alpha(s)=\mathbf P(f_s(X)=1|Y=0)=P(S(X) \geq s|Y=0)
$$

and

$$
\beta(s)=\mathbf P(f_s(X)=0|Y=1)=P(S(X) < s|Y=1)
$$

$\alpha(s)$ is called **false positive** rate and $\beta(s)$ **false negative** rate. Similarly **specificity** and **sensitivity** are defined:

$$
sp(s)=P(S(X) < s|Y=0) = 1 - \alpha(s)
$$

and

$$
se(s)=P(S(X) \geq s|Y=1) = 1 - \beta(s)
$$

The ROC (Receiver Operating Characteristic) curve allows to visualize $\alpha$ and $\beta$ on a same graph for all $s$ allowing to choose the cutoff and compare different Scores.

Precisely, the ROC curve of a Score $S$ is a parametric curve of the variable $s$:

$$
\begin{array}{ll} 
    ROC:&\mathbb R \to [0,1]^2 \cr
        & s \to (x(s)=\alpha(s),y(s)=1-\beta(s))\cr
\end{array}
$$

For a given Score $S$, the ROC curve passes through the points $(0,0)$ and $(1,1)$, which corresponds to classifying all observations as $0$ ($s\to \infty$) or $1$ ($s\to -\infty$).

We now define two special cases:

A Score $S$ is said to be **perfect** if $s^*$ exists such that:

$$
P(Y=1|S(X) \geq s^*) = 1
$$ and $$
P(Y=0|S(X) < s^*) = 1
$$

It corresponds to $x(s^*)=0$ and $y(s^*)=1$ (using the preceding definitions and Bayes rule).

For example:

$$
x(s^*)=P(S(X) \geq s^*|Y=0)=\frac{P(Y=0|S(X) \geq s^*)P(S(X) \geq s^*)}{P(Y=0)}=0
$$

Similarly: for $s \leq s^*$ we have $x(s)=0$ and for $s > s^*$ we have $y(s)=1$.

A score $S$ is said to be **random** if $S(X)$ and $Y$ are independent. In such case:

$$
x(s)=P(S(X) \geq s|Y=0)=P(S(X))=P(S(X) \geq s|Y=1)=y(s)
$$

The ROC curve for a random score is the first bisector $(0,0)$ to $(1,1)$.

@fig-roc-perfect-random shows the two curves.

```{r, warning=FALSE}
#| label: fig-roc-perfect-random
#| fig-cap: "ROC curves of a perfect Score (solid, blue) and a random Score (dashed, orange)"


perfect_score_label = "s=s^{\\*}"
temp <- expression("s="~rho == 0.34)

ggplot() +
    geom_segment(aes(x = 0, y = 0, xend = 0, yend = 1), col="dodgerblue", linewidth = 2) + # perfect score vert
    geom_segment(aes(x = 0, y = 1, xend = 1, yend = 1), col="dodgerblue", linewidth = 2) + # perfect score horiz
    geom_segment(aes(x = 0, y = 0, xend = 1, yend = 1), col="orange", linetype = 2, linewidth = 1.5) + # random score
    # annotate perfect score
    annotate("segment", x = 0.05, y = 0.95, xend = 0, yend = 1,
             arrow = arrow(type = "closed", length = unit(0.02, "npc"))) +
    annotate("text", x=0.08,y=0.92, label = 's == "s*"', parse=TRUE) +
    # -Inf
    annotate("segment", x = 0.95, y = 0.95, xend = 1, yend = 1,
             arrow = arrow(type = "closed", length = unit(0.02, "npc")))+
    annotate("text", x=0.92,y=0.92, label = expression(s==-infinity), parse=TRUE) +
    # +Inf
    annotate("segment", x = 0.05, y = 0.05, xend = 0, yend = 0,
             arrow = arrow(type = "closed", length = unit(0.02, "npc")))+
    annotate("text", x=0.08,y=0.08, label = expression(s==infinity), parse=TRUE) +
    coord_cartesian(xlim = c(0, 1), ylim = c(0, 1), expand = FALSE) +
    labs(x = "false positives", y = "true positives") +
    theme_bw() 

```

The ROC curves can be summarized to generate numeric criteria such as the AUC (Area Under Curve).

The AUC will be $1$ for the perfect Score and $0.5$ for the random Score and can be used to rapidly compare two Scores.

It is considered better than other point-wise criteria.

However we must keep in mind that the AUC as a summary criterion presents some drawbacks since two Scores can have the same AUC but behave very differently in the plane.

In practice we do not know the probabilities $x(s)$ and $y(s)$ to define the ROC curve.

As said before, a training set will be used to learn a Score $S$ and a testing set to estimate the ROC curve $x(s)$ and $y(s)$ for a range of $s$.

In the rest of the course we will be equally interested in binary classification and scoring which are closely linked.

Throughout the course, we will focus on two fundamental and complementary models: the Logistic Regression model and Decision Trees.

Logistic Regression employs a "generative" approach to model with a parametric law the joint probability of input $X$ and outputs $Y$ and then estimate the parameters of the regression function.

On the other hand, Decision Trees adopt a "machine learning" approach, aiming to minimize empirical risk. These models serve as the foundation elements for modern machine learning algorithm, making them essential components of the classification/scoring toolkit.

Moreover these two methods can be combined, see for example this [blog post from Criteo engineering team on combining Logistic Regression and Decision Trees in production](https://medium.com/criteo-engineering/unity-is-strength-a-story-of-model-composition-49748b1f1347).

# The Logistic Regression model

## Informal introductory example

We provide here some intuitions leading to the Logistic Regression model using a simulated data set from @islr2021 (the **Default** data set).

This is a toy data set used for teaching purposes containing information on ten thousand customers.

The aim here is to assess which customers will ***default*** on their credit card debt (the target or response variable) based on the current credit card ***balance*** and other individual characteristics (the predictors or feature vector).

This is a binary classification problem as the ***default*** variable takes value in a discrete set (here binary). In the following we will denote $Y$ the output or response variable and $X = (X_0,X_1,\cdots,X_{p-1})^{T}$ the feature vector or inputs.

The approach we follow is similar to [@hosmer2013] or [@cornillon2019]. Both books use a similar example: the presence or absence of a Coronary Heart Disease (CHD) is explained with the age of an individual (the data set *chdage* is available in companion package *aplore3*).

We can start to explore the Default data with a scatterplot (@fig-default_balance-scatterplot) of the target variable (***default***) with respect to a predictor (***balance***):

```{r}
#| label: fig-default_balance-scatterplot
#| fig-cap: "Scatterplot of variable ***default*** with respect to credit card ***balance*** for 10000 customers"

# Default data set (simulated) from ESLII/ISLR
default_data <- ISLR2::Default %>%
    as_tibble()

ggplot(default_data, aes(x=balance, y=default)) +
geom_point(alpha=0.2)
```

In this scatterplot, all points fall on one of two parallel lines representing the absence (No) or occurrence (Yes) of ***default***. We "jitter" the data vertically to avoid overplotting. The plot below shows that the response variable is imbalanced towards the absence of default:

```{r}
ggplot(default_data, aes(x=balance, y=default)) +
geom_jitter(alpha=0.2, height=0.2)
```

We also show the boxplots of credit cards ***balance*** with respect to ***default*** status:

```{r}
#| label: fig-balance_default-boxplot
#| fig-cap: "Variable ***balance*** with respect to ***default*** status"
ggplot(default_data,
       aes(x=default, y=balance)) +
geom_boxplot()
```

We can see from @fig-default_balance-scatterplot and @fig-balance_default-boxplot that default tends to be more prevalent for accounts with a high balance. However it is difficult to guess a simple relationship between default and balance.

To investigate further we discretise the balance variables by classes of width $300\$$ and compute the mean of response variable (***default*** is Yes) within each balance class:

```{r, message = FALSE}
class_width <- 300
(default_data_binned <- default_data %>%
    mutate(balance_bins = cut(balance, breaks = seq(0, 3000, class_width),
                              right = FALSE, dig.lab = 4),
           min = floor(balance / class_width) * class_width,
           max = if_else(balance == 0 , 1, 
                         # customers with 0$ balance should be long to [0, width) class
                         # or be excluded
                         ceiling(balance / class_width))  * class_width) %>% 
    group_by(balance_bins, min, max, default) %>% 
    summarize(n=n()) %>% 
    ungroup() %>% 
    pivot_wider(names_from = default, values_from = n) %>%
    replace_na(list(Yes = 0, No = 0)) %>% 
    mutate(`Mean(default)` = round(Yes / (Yes + No), 4)))
```

In the following we map, by convention and for better readability, the response variable $Y \in\{Yes, No\}$ to $\{0, 1\}$:

$$
Y = \left\{ \begin{array}{ll} 
    1&  \mbox{if customer defaulted on its credit card (ie default=Yes)}\cr
    0&  \mbox{otherwise}.
    \end{array} \right.
$$

Then we plot the mean of default (in red) within each balance class (of width $300\$$):

```{r}
#| label: fig-balance_default-scatterplot-occurrence
#| fig-cap: "Mean occurrence of ***default*** within ***balance*** classes"

(default_occurrence <- 
 ggplot(default_data %>% mutate(default = if_else(default == "Yes", 1, 0)), aes(x=balance, y=default)) +
 geom_point(alpha=0.2) +
 geom_segment(data = default_data_binned,
              aes(x = min, xend = max, y = `Mean(default)`, yend = `Mean(default)`),
              color = 'coral',
              linewidth=1.25))
```

The relationship between the mean occurrence of ***default*** and ***balance*** is easier to read.

@fig-balance_default-scatterplot-occurrence clearly shows that as balance increases, the proportion of customers defaulting on their credit card increases.

We also notice that the mean default occurrence with respect to balance classes follows a kind of "S"-shaped curve or **sigmoid** function. Note that the shape depends on classes width and might change.

Going further and informally, considering that the mean of default occurrence is an estimate of $\mathbf{E}[Y|X=x]$ for each balance classes an idea would be to model:

$$
 \mathbb{E}[Y|X=x] = \mu_\beta(x)
$$

where $\mu_\beta$ is a **sigmoid** function in $[0,1]$.

The Logistic Regression model uses the **sigmoid** function $\sigma: x \to\sigma(x)=\frac{e^{x}} { 1 + e^{x} }$ also known as logistic function.

```{r}
(default_occurrence +
 geom_smooth(method = "glm", 
             formula = y ~ x,
             method.args = list(family = "binomial"), 
             se = FALSE,
             col = "dodgerblue",
             linetype = "dotted") +
 geom_segment(data = default_data_binned,
              aes(x = min, xend = max, y = `Mean(default)`, yend = `Mean(default)`),
              color = 'coral',
              linewidth=1.25))
```

Another approach would have been to treat $Y$ as a quantitative response variable and fit a simple linear model:

```{r}
linear <- lm(default ~ balance, data = default_data %>% mutate(default = if_else(default == "Yes", 1, 0)))
#summary(linear)
coeff_lm <- linear$coefficients
alpha <- coeff_lm["(Intercept)"]
beta <- coeff_lm["balance"]
```

```{r, warning = FALSE}
#| label: fig-balance_default-scatterplot-lmfit
#| fig-cap: "Fitting a linear model, ***default*** is treated as a quantitative response variable"

(ggplot(default_data %>% mutate(default = if_else(default == "Yes", 1, 0)), aes(x=balance, y=default)) +
 geom_point(alpha=0.2) +
 geom_segment(data = default_data_binned,
              aes(x = min, xend = max, y = `Mean(default)`, yend = `Mean(default)`),
              color = 'coral',
              size=1.25)+
 geom_line(data = tibble(x = seq(0,3000, 300)) %>%
               mutate(y = alpha + beta * x),
           aes(x = x, y = y),
           color = 'darkolivegreen',
           linetype = 'dotted'))
 # Alternatively we could have used the geom_smooth command
 # geom_smooth(method = "lm", 
 #             formula = y ~ x,
 #             se = FALSE,
 #             col = "darkolivegreen",
 #             linetype = "dotted")

```

@fig-balance_default-scatterplot-lmfit shows that a linear model fails to fit the data. In particular, for low credit card balances the linear model shows a "negative probability of default". This is quite prominent here as response variable is imbalanced towards the No default category. For the same reasons, the least square method fails to correctly fit the category of interest (less than 0.25 probability).

Usually in such presentation (for example @hosmer2013) data is more balanced and a linear model approximately fits the two classes. However in any case a linear model cannot confine the predicted value to $[0, 1]$ for all observations of predictors.

## A more formal definition

We use the concepts we have defined in the first part of this lesson. The problem we are facing trying to predict the output default ($Y \in(0,1)$) using a training set of inputs or featur vector $X$ is a binary classification problem.

We remind that to estimate an optimal classifier for output $Y \in \{0,1\}$ using input $X = (X_1,\cdots,X_p)$ one approach was to:

-   model the joint distribution (or generative distribution) between $X$ and $Y$,

-   estimate $\eta(x)$ with:

$$
\eta(x)=\mathbb{P}[Y=1|X=x)] = \mathbb{E}[Y=1|X=x)]
$$

-   $\eta(x)$ can be used as a Scoring function (ROC curve, choice of cutoff $s$)

-   and we can use the classifier (usually $s=\frac{1}{2}$) to predict output $Y$:

$$
f_s(x)=\left\{ \begin{array}{ll} 
    1 &  \mbox{if } \eta(x)\geq s\cr
    0 &  \mbox{otherwise}\cr
\end{array} \right.
$$

In the case of **Logistic Regression** classifier, we model:

$$
Y|X=x~ \sim B(\eta(x))
$$

with

$$
\eta(x)=\sigma(x^T\beta) =\frac{exp(x^T\beta)}{1+exp(x^T\beta)}
$$

for some parameter $\beta=(\beta_1,\cdots,\beta_p)\in \mathbb R^p$, usually $x_1=1$ and $\beta_1$ is an intercept. $\sigma$ is the sigmoid logistic function we have seen before.

In the literature is usual to denote $\eta(X)=p_{\beta}(X)$ or $\eta(X)=\pi_{\beta}(X)$.

From now, we will use the notation $p_{\beta}(X)$.

Defining $\mathrm{logit}: x \to \log\bigg( \frac{x}{1-x}\bigg)$ we have:

$$
\mathrm{logit}(p_{\beta}(X))=X^T\beta
$$

::: {.callout-tip icon="false"}
## The **Logistic Regression** model:

We are given $(x_i, y_i) \in \mathbb R^p \times \{0,1\}$, $i=1,\cdots,n$

The Logistic Regression model assumes that outputs $y_i$ are independent Bernoulli with parameter $p_{\beta}(x_i)$ depending on $x_i$:

$$
\mathrm{logit}(p_{\beta}(x_i))=x_i^T\beta
$$
:::

The Logistic Regression model defined above is a special case of more general family of models, the so-called Generalized Linear Models (GLM).

The family of GLM extends the applicability of linear-model ideas to data where responses are binary (e.g. Logistic Regression) or counts (e.g. Poisson Regression), among further possibilities. The concept emerged with Nelder and Wedderburn and has been studied extensively (see @nelder1972 or @cornillon2019) .

The syntax to fit the Logistic model in R using `glm()` is:

$$
\mathtt{glm(} \mathrm{y} \sim \mathrm{~x,~}\mathtt{data=}~\mathrm{dataframe,~}\mathtt{family~=~binomial(link~=~"logit")}
$$

The formula $\mathrm{y} \sim \mathrm{x}$ depicts the model (i.e. inputs are $X$, output is $Y$) and the `data=` argument points to the training set contained in a R dataframe (or tibble). This is quite similar to the `lm()` function.

We also need to specify the distribution for the conditional $Y$ values (binomial) and the link function (logit) via the `family=` argument.

For our example:

```{r}
#| code-fold: show
glm_default <- glm(default ~ 1 + student + balance + income,
                   data = default_data,
                   family = binomial(link = "logit"))

glm_default <- glm(default ~ .,
                   data = default_data,
                   family = "binomial") # by default: link = "logit"
 
```

The command `summary` produces result summaries of the fitted model:

```{r}
#| code-fold: show
summary(glm_default)
```

We will see later how to interpret or understand what is printed

A coefficient-wise output of the model can be obtained as a `tibble` using `tidy()` from package `broom`:

```{r}
broom::tidy(glm_default)
```

Based on this output, the fitted model is (we have re-scaled balance and income for better readability):

$$
\log \bigg( \frac{p_{\beta}(x_i)}{1 - p_{\beta}(x_i)}\bigg) = -1.08 - 0.65(\mathbb{1}_{\mathrm{student}_i=\mathrm{Yes}}) + 5.74(\frac{\mathrm{balance}_i}{1000})+ 0.03(\frac{\mathrm{income}_i}{10000})
$$

## Estimation

### Maximum Likelihood Estimation

We are given $(x_i, y_i) \in \mathbb R^p \times \{0,1\}$, $i=1,\cdots,n$ where outputs $y_i$ are independent Bernoulli with parameter $p_{\beta}(x_i)$ depending on $x_i$:

$$
\mathrm{logit}(p_{\beta}(x_i))=x_i^T\beta
$$

The parameters $\beta$ of the Logistic Regression model are usually determined using Maximum Likelihood Estimation (MLE). It consists on finding $\beta$ for which the joint probability of the observed data is greatest.

As $y_i$ are independent the likelihood function (joint probability) is the product of the probability mass functions:

$$
L(Y,\beta) = \prod_{i=1}^n p_{\beta}(x_i)^{y_i}(1-p_{\beta}(x_i))^{1-y_i}
$$ with $Y=(y_1,\cdots,y_n)$ and $\beta=(\beta_1,\cdots,\beta_p)$.

We seek to maximize the likelihood function over $\beta$, it is equivalent but easier to maximize the log-likelihood:

$$
\begin{align}
\ell(Y,\beta)=\log L(Y,\beta) &=\sum_{i=1}^n \left(y_i \log(p_{\beta}(x_i))+(1-y_i) \log(1- p_{\beta}(x_i))\right) \\
                              &=\sum_{i=1}^{n} \left(y_i \log(\frac{p_{\beta}(x_i)}{1-p_{\beta}(x_i)})+\log(1- p_{\beta}(x_i))\right) \\ 
                              &=\sum_{i=1}^{n} \left(y_i x_i^T\beta -\log(1 + \exp(x_i^T\beta)\right)
\end{align} 
$$

If the MLE $\hat\beta$ exists, the gradient of log-likelihood satisfies (first order necessary condition):

$$
\nabla\ell(Y,\beta)=\left(\frac{\partial \ell(Y,\beta)}{\partial \beta_1}, \cdots,\frac{\partial \ell(Y,\beta)}{\partial \beta_p}\right)=\mathbf 0
$$

We have for $j=1,\cdots,p$:

$$
\frac{\partial \ell(Y,\beta)}{\partial \beta_j}=\sum_{i=1}^{n} \left(y_i x_{ij} -x_{ij}\frac{\exp(x_i^T\beta)}{1+\exp(x_i^T\beta)}\right)=\sum_{i=1}^{n} x_{ij} \left(y_i- p_{\beta}(x_i)\right)
$$

In vector form:

$$
\nabla\ell(Y,\beta)=\sum_{i=1}^{n} x_{i} \left(y_i- p_{\beta}(x_i)\right)=X^T(Y-P_{\beta})
$$

where:

$$
\begin{align}
X = \begin{pmatrix}
x_{11} & \cdots & x_{1p} \\
x_{21} & \cdots & x_{2p} \\
\vdots  & \vdots  & \vdots   \\
x_{n1} & \cdots & x_{np} 
\end{pmatrix} = 
\begin{pmatrix}
 x_1^T\\
 x_2^T\\
\vdots \\
 x_n^T
\end{pmatrix}\in \mathbb{R}^{n\times (p)}, \quad
Y = \begin{pmatrix}
y_{1} \\
y_{2}\\
\vdots \\
y_{n}  
\end{pmatrix} \quad and \quad
P_{\beta} = \begin{pmatrix}
p_{\beta}(x_1) \\
p_{\beta}(x_2)\\
\vdots \\
p_{\beta}(x_n)  
\end{pmatrix} 
\end{align}
$$

In the literature $\nabla\ell(Y,\beta)$ is denoted as the Fisher's score function $S(\beta)$, if the MLE $\hat\beta$ exists, we have:

$$
S(\hat\beta)=\nabla\ell(Y,\hat\beta)=X^T(Y-P_{\hat\beta})=0
$$

Solving this equation involves solving $p$ non-linear equations in $\beta$:

$$
y_1 x_{1j} + \cdots + y_n x_{nj} = x_{1j}\frac{\exp(x_1^T\beta)}{1+\exp(x_1^T\beta)}+ \cdots + x_{nj}\frac{\exp(x_n^T\beta)}{1+\exp(x_n^T\beta)},\quad j=1,\cdots,p
$$

### Numerical methods

In practice we use numerical methods to solve these non-linear equations as no closed-form solution exist.

If we assume that $rank(X)=p$, we will have that $S(\beta)$ is concave in $\beta$ hence if we find a local maximum it is a global maximum.

We have for $(k,l) \in (1,\cdots,p)^2$:

$$
\begin{align}
\frac{\partial\mathcal \ell}{\partial\beta_k\partial\beta_l}(\beta)= & \frac{\partial}{\partial\beta_k}
\sum_{i=1}^nx_{il}(y_i-\frac{\exp(x_i^T\beta)}{1+\exp(x_i^T\beta)}) \\
=& -\sum_{i=1}^nx_{il}x_{ik}\frac{\exp(x_i^T\beta)}{(1+\exp(x_i^T\beta))^2} \\
=& -\sum_{i=1}^nx_{ik}p_\beta(x_i)(1-p_\beta(x_i))x_{il}
\end{align}
$$

We obtain that in matrix form:

$$
H(\beta)=\nabla^2\ell(Y,\beta)=-X^T W_\beta X
$$

where:

$$
\begin{align}
W_\beta = \begin{pmatrix}
p_\beta(x_1)(1-p_\beta(x_1)) & \cdots & \cdots\\
\vdots  & \ddots & \vdots \\
\cdots  & \cdots & p_\beta(x_n)(1-p_\beta(x_n))
\end{pmatrix}
\end{align} 
$$

We have $p_\beta(x_i)(1-p_\beta(x_i))\geq0$ hence $W(\beta)$ is semi-definite negative and since $rank(X)=p$, $H(\beta)$ is concave.

It is shown in [@albert1984a] that if additionally there is no complete separation in the training set: ![](images/unique_mle.png){fig-align="center"}

then the MLE exists and is unique.

#### The Newton-Raphson (ie Fisher Scoring) method

In practice the Newton-Raphson method is used to solve the equation:

$$
S(\beta)=\nabla\ell(Y,\beta)=X^T(Y-P_{\beta})=0
$$

Using Taylor expansion of Score $S(\beta)$:

$$
S(\hat\beta) \approx S(\beta^{(k)})+H(\beta^{(k)})(\hat\beta-\beta^{(k)})
$$ and starting from an initial guess of $\beta=\beta_0$, the Newton-Raphson update formula is:

$$
\beta^{(k+1)} = \beta^{(k)} - H^{-1}(\beta^{(k)})S(\beta^{(k)})
$$

We show below a naive implementation of Newton-Raphson method to estimate $\beta$ (also known as Fisher Scoring algorithm in the context of Logistic Regression)

```{r}
#| code-fold: show
# We put the data frame in matrix form
# also adding an intercept
X <- cbind(rep(1, nrow(default_data)),
                          as.matrix(default_data %>% select(balance, income))) 
colnames(X) <-  c("(Intercept)", "balance", "income")
n <- nrow(X)

# We extract the output as vector
Y <- default_data %>% mutate(default = if_else(default=='Yes', 1, 0)) %>% pull(default)


# We set an initial guess for beta and criterion for stopping
beta <- c(0.01, 0.0, 0.0)
nb_iter <- 25
tol <- 1e-4

lr_solve <- function(X, Y, beta, nb_iter, tol){
    for(i in 1:nb_iter){
        # first compute p_beta(X)
        p_beta <- exp(X %*% beta) / (1 + exp(X %*% beta))
        
        # then the Score
        Score_beta <- t(X) %*% (Y-p_beta)
        
        # and the Hessian
        W_beta <- matrix(0, n, n)
        diag(W_beta) <- p_beta*(1-p_beta)
        
        Hessian_beta <- -t(X) %*% W_beta %*% X
        
        # we update beta
        new_beta <- beta - solve(Hessian_beta) %*% Score_beta
        
        # we check for convergence
        if(t(beta-new_beta) %*% (beta-new_beta) < tol){
            return(list(beta = beta, hessian = Hessian_beta, nb_iter = i)) 
        }
        beta <- new_beta
    }
    return(list(beta = beta, hessian = Hessian_beta, nb_iter = i)) 
}

sol <- lr_solve(X, Y, beta, nb_iter, tol)
```

We verify that R `glm()` and our algorithm give the same coefficients:

-   Newton-Raphson:

```{r}
print(sol$beta)
```

-   R `glm()`:

```{r}
glm_bal_inc <- glm(default ~ balance + income,
                 data = default_data,
                 family = "binomial")

print(coef(glm_bal_inc ))

```

In the next sections (Interpretation, Confidence intervals, Tests) we will try to understand the outputs of the `glm()` function from a statistical viewpoint.

#### The Iterative Reweighted Least Square (IRLS) method

There is an equivalent approach to the the Newton-Raphson described in the literature as Iterative Reweighted Least Square (IRLS).

The Newton-Raphson update formula rewrites:

$$
\begin{align}
\beta^{(k+1)} &= \beta^{(k)} - H^{-1}(\beta^{(k)})S(\beta^{(k)}) \\
&=\beta^{(k)} + (X^TW_{\beta^{(k)}}X)^{-1}X^T(Y-P_{\beta^{(k)}}) \\
&= (X^TW_{\beta^{(k)}}X)^{-1}(X^TW_{\beta^{(k)}}X)\beta^{(k)} + (X^TW_{\beta^{(k)}}X)^{-1}X^T(Y-P_{\beta^{(k)}}) \\ 
&= (X^TW_{\beta^{(k)}}X)^{-1}X^TW_{\beta^{(k)}}\left(X\beta^{(k)}+W_{\beta^{(k)}}^{-1}(Y-P_{\beta^{(k)}}) \right) \\
&= (X^TW_{\beta^{(k)}}X)^{-1}X^TW_{\beta^{(k)}}Z_{\beta^{(k)}} \\
\end{align}
$$

where:

$$
Z_{\beta^{(k)}}= X\beta^{(k)}+W_{\beta^{(k)}}^{-1}(Y-P_{\beta^{(k)}})
$$

$\beta^{(k+1)} = (X^TW_{\beta^{(k)}}X)^{-1}X^TW_{\beta^{(k)}}Z_{\beta{(k)}}$ corresponds to the solution of a weighted ($W_{\beta^{(k)}}$) linear regression of $Z_{\beta^{(k)}}$ by $X$. As an exercise you can implement this algorithm.

#### Example where MLE is not finite

To conclude on the numerical aspects, we signal a special and extreme case where the iterative algorithm won't converge. The theoretical aspect is covered in [@albert1984a].

We simulate a perfectly separated data set. Here $X\in \mathbb [-1,1]$ and $Y\in\{0,1\}$:

```{r}
set.seed(1987)
X <- c(runif(n = 50, min = -1, max = 0),
       runif(n = 50, min = 0, max = 1))
Y <- c(rep(0, 50), rep(1, 50))

tbl_separated <- tibble(X,Y)
ggplot(tbl_separated) + geom_point(aes(X, Y))
```

In this setting the iterative algorithm fails to converge and coefficient "saturates" to a high/low value (while it should go to infinite):

```{r}
glm_separated <- glm(Y ~ X,
                     data = tbl_separated,
                     family="binomial")

glm_separated$coef
```

```{r}
separated_fit <- broom::augment(glm_separated, type.predict = "response")
ggplot(separated_fit) +
    geom_point(aes(X, Y)) +
    geom_line(aes(X,.fitted))
```

Now we slightly modify the data set, changing a $y$ observation with $x\in[-1,0]$ from $0$ to $1$.

```{r}
Y1 <- Y
Y1[25] <- 1
tbl_overlap <- tibble(X,Y1)
ggplot(tbl_overlap) + geom_point(aes(X, Y1))
```

The iterative algorithm converges again and the impact on the Scoring function (i.e. $\mathbb{P}[Y=1|X=x)$) and the decision rule (shifting left below $X=O$) is not negligible for a one point change:

```{r}
glm_overlap <- glm(Y1 ~ X,
                     data = tbl_separated,
                     family="binomial")
overlap_fit <- broom::augment(glm_overlap, type.predict = "response")
ggplot(overlap_fit) +
    geom_point(aes(X, Y1)) +
    geom_line(aes(X,.fitted))
```

Nonetheless the case we described is very unlikely to happen in a real life setting, and a good data set exploration should avoid such trap. More details can be found in the document `Separation and Convergence Issues in Logistic Regression.pdf`

### Logistic Regression as a machine learning approach

We have another look at the log-likelihood equation stated before, we have:

$$
\begin{align}
\ell(Y,\beta)=\log L(Y,\beta) &=\sum_{i=1}^n \left(y_i \log(p_{\beta}(x_i))+(1-y_i) \log(1- p_{\beta}(x_i))\right) \\
                              &=-\sum_{i=1}^{n} \ell_{logistic} \left(p_{\beta}(x_i),y_i\right) \\
                              &= -n\hat{\mathrm R}(p_\beta) 
\end{align} 
$$

where $\ell_{logistic}: \{0,1\}\times \{0,1\} \to \mathbb R^+$:

$$
\ell_{logistic}(y,z) = -y\log(z)-(1-y)\log(1-z)=\left\{ \begin{array}{ll} 
    -\log(z) &  \mbox{if } y = 1\cr
    -\log(1-z) &  \mbox{if } y = 0\cr
\end{array} \right.
$$

and $\hat{\mathrm {R}}(p_\beta) $ is the empirical risk on the training set.

Estimating $\beta$ by maximizing the log-likelihood is equivalent to minimizing with respect to $\beta$ the empirical risk of $p_\beta$ for the logistic loss. Note that usually in the context of machine learning and logistic loss, the output $Y$ is relabeled to $\{-1,1\}$


## Real life data sets

### The **German Credit** (inspired from S. Tufféry)

https://archive.ics.uci.edu/ml/datasets/statlog+(german+credit+data) https://www.r-bloggers.com/2016/03/classification-on-the-german-credit-database/ Also available in scorecard package

```{r, message=FALSE}
load('../data/germancredit.RData')

germancredit_clean <- germancredit %>%
    as_tibble() %>% 
    rename_all(~ stringr::str_replace_all(., "\\.", "_")) %>% 
    mutate(creditability = as.factor(as.numeric(creditability) - 1)) %>%  # map target to 0 (good credit) 1 (bad credit)
           # age_binned = cut(age_in_years, c(0, 25, Inf)),
           # credit_amount_binned = cut(credit_amount, c(0, 2500, Inf)),
           # duration_binned = cut(duration_in_month, c(0, 15, 36, Inf))) %>% 
    rename(status = status_of_existing_checking_account,
           duration = duration_in_month,
           history = credit_history,
           amount = credit_amount,
           savings = savings_account_and_bonds,
           job_since = present_employment_since,
           icr = installment_rate_in_percentage_of_disposable_income,
           personal = personal_status_and_sex,
           resid_since = present_residence_since,
           age = age_in_years,
           debtors_guarantors = other_debtors_or_guarantors,
           existing_credits = number_of_existing_credits_at_this_bank,
           to_provide = number_of_people_being_liable_to_provide_maintenance_for
           )
    
glimpse(germancredit_clean)
```

Description of the data

```{r}
vars_quanti <- names(germancredit_clean %>% select_if(is.numeric))
for(var in vars_quanti){
    var <- as.name(var)
    print(ggplot(germancredit_clean %>% mutate(creditability = as.numeric(creditability)-1), aes(x=!! var,y = creditability)) +
        geom_jitter(height = 0.1, width = 0) +
        geom_smooth(method = "glm", 
                    formula = y ~ x,
                    method.args = list(family = "binomial"), 
                    se = FALSE,
                    col = "dodgerblue"))
}
```

```{r}
germancredit_clean <- germancredit_clean %>%
    mutate(icr = as.factor(icr),
           resid_since = as.factor(resid_since),
           existing_credits = as.factor(existing_credits),
           to_provide = as.factor(to_provide)
           )
```

```{r, message = FALSE}
vars_quali <- names(germancredit_clean %>% select_if(is.factor) %>% select(-creditability))
for(var in vars_quali){
    var <- as.name(var)
    print(ggplot(germancredit_clean %>% 
                     group_by(!!var, creditability) %>% 
                     summarize(count = n()) %>% 
                     ungroup()) +
        geom_bar(aes(x = creditability, y = count, fill = !!var), position="dodge",stat="identity"))
}
```

```{r, message = FALSE}
germancredit_glm <- glm(creditability ~ . , data = germancredit_clean, family = "binomial")
summary(germancredit_glm)
```

### Agriculture Farm Lending

```{r}
# package used to import spss file
library(foreign)

don_desbois <- read.spss("../data/Agriculture Farm Lending/desbois.sav",
                       to.data.frame = TRUE) %>% as_tibble()
don_desbois <- don_desbois %>%
    mutate(Y = as.factor(if_else(DIFF=='healthy', 0, 1))) %>%
    dplyr::select(-DIFF)

model_desbois <- glm(Y ~ STATUS + HECTARE + r1 + r3 + r17 + r24 + r28 + r36,
                     data = don_desbois,
                     family = "binomial",
                     maxit = 100)

summary(model_desbois)
```


```{r}
model_desbois <- glm(Y ~ OWNLAND + r17,
                     data = don_desbois,
                     family = "binomial")

summary(model_desbois)

```


## Exercise

Using the Mixture data set, plot ROC curves for some chosen classifiers:

* implementing from scratch a method for plotting using the course description

```{r}
#| code-fold: show
roc_curve <- function(s, error_rate){
    # error_rate is a tibble containing a column Y of true values and a column score 
    
    # let s be a threshold in R
    output <- factor(error_rate %>% pull(Y), levels = c(0,1))
    
    classifier <- error_rate %>%
        mutate(classifier = factor(if_else(score>=s, 1, 0), levels = c(0,1))) %>% 
        pull(classifier)

    # compute confusion matrix and ROC curve coordinates
    conf_mat <- table(output, classifier)
    #       classifier
    # output    0       1
    #      0 3681(TN) 1319(FP)
    #      1 1586(FN) 3414(TP)
    
    true_negative <- conf_mat[1,1]
    false_positive <- conf_mat[1,2]
    
    false_negative <- conf_mat[2,1]
    true_positive <- conf_mat[2,2]
    
    false_positive_rate <- false_positive / (false_positive + true_negative)
    
    true_positive_rate <- true_positive / (false_negative + true_positive) # aka sensitivity
    
    return(tibble(threshold = s, fpr = false_positive_rate, tpr = true_positive_rate))
}
```

```{r}
#| code-fold: show
# In the case of logistic regression, we use the regression function in [0, 1] as a scoring function
# We vary s in [0, 1] to compute the ROC parametric curve 
roc_coordinates_logit <- list()

for (s in seq(0, 1, 0.001)){
    roc_coordinates_logit <- append(roc_coordinates_logit,
                                    list(roc_curve(s, logit_error_rate %>% mutate(score = .fitted))))
}
roc_coordinates_logit <- bind_rows(roc_coordinates_logit)

ggplot(roc_coordinates_logit, aes(x=fpr, y = tpr)) + geom_line()

```
```{r}
#| code-fold: show
# In the case of logistic regression, we use the regression function in [0, 1] as a scoring function
# We vary s in [0, 1] to compute the ROC parametric curve 
roc_coordinates_logit_splines <- list()

for (s in seq(0, 1, 0.001)){
    roc_coordinates_logit_splines <- append(roc_coordinates_logit_splines,
                                    list(roc_curve(s, logit_splines_add_error_rate %>% mutate(score = .fitted))))
}
roc_coordinates_logit_splines <- bind_rows(roc_coordinates_logit_splines)

ggplot(roc_coordinates_logit_splines, aes(x=fpr, y = tpr)) + geom_line()


```
```{r}
#| code-fold: show
# In the case of logistic regression, we use the regression function in [0, 1] as a scoring function
# We vary s in [0, 1] to compute the ROC parametric curve 
roc_coordinates_logit_binned <- list()

for (s in seq(0, 1, 0.001)){
    roc_coordinates_logit_binned <- append(roc_coordinates_logit_binned,
                                    list(roc_curve(s, logit_binned_error_rate %>% mutate(score = .fitted))))
}
roc_coordinates_logit_binned <- bind_rows(roc_coordinates_logit_binned)

ggplot(roc_coordinates_logit_binned, aes(x=fpr, y = tpr)) + geom_line()
```

```{r}
#| code-fold: show
# In the case of cart, we use the predicted prob in [0, 1] as a scoring function
# Caution, with decision trees, the "probability" or score function is given by the appartenance to a terminal node
# In our case we have 6 terminal nodes a
# We vary s in [0, 1] to compute the ROC parametric curve 
roc_coordinates_cart<- list()

for (s in seq(0, 1, 0.001)){
    roc_coordinates_cart <- append(roc_coordinates_cart,
                                    list(roc_curve(s, cart_error_rate %>% mutate(score = prob))))
}
roc_coordinates_cart <- bind_rows(roc_coordinates_cart)

ggplot(roc_coordinates_cart, aes(x=fpr, y = tpr)) + geom_line()

```
```{r}
#| code-fold: show
ggplot() +
    geom_line(data = roc_coordinates_logit, aes(x=fpr, y = tpr, color = 'logistic')) +
    geom_line(data = roc_coordinates_logit_splines, aes(x=fpr, y = tpr, color = 'logistic splines')) +
     geom_line(data = roc_coordinates_logit_binned, aes(x=fpr, y = tpr, color = 'logistic binned')) +
    geom_line(data = roc_coordinates_cart, aes(x=fpr, y = tpr, col = 'decision tree')) +
    scale_color_manual(name='Model',
                     breaks=c('logistic', 'logistic splines', 'logistic binned', 'decision tree'),
                     values=c('logistic'='darkorange', 
                              'logistic splines' = 'darkolivegreen',
                              'logistic binned' = 'plum4',
                              'decision tree'='dodgerblue'))


```

* using ROCR package (see `ROCR_Talk_Tobias_Sing.ppt` for a presentation)

```{r}
#| code-fold: show
# ROC Curves with ROCR
library("ROCR")    

# logit
pred <- prediction(logit_error_rate$.fitted, logit_error_rate$Y)
perf <- performance(pred, measure = "tpr", x.measure = "fpr")
plot(perf, main="ROC curve Admissions", xlab="Specificity",
     ylab="Sensitivity", col = "darkorange")
abline(0, 1) #add a 45 degree line

# logit splines
pred <- prediction(logit_splines_add_error_rate$.fitted, logit_splines_add_error_rate$Y)
perf <- performance(pred, measure = "tpr", x.measure = "fpr")
plot(perf, add = TRUE, main="ROC curve Admissions", xlab="Specificity",
     ylab="Sensitivity", col = "darkolivegreen")

# logit binned
pred <- prediction(logit_binned_error_rate$.fitted, logit_binned_error_rate$Y)
perf <- performance(pred, measure = "tpr", x.measure = "fpr")
plot(perf, add = TRUE, main="ROC curve Admissions", xlab="Specificity",
     ylab="Sensitivity", col = "plum4")

# CART
pred <- prediction(cart_error_rate$prob , cart_error_rate$Y)
perf <- performance(pred, measure = "tpr", x.measure = "fpr")
plot(perf, add = TRUE, main="ROC curve Admissions", xlab="Specificity",
     ylab="Sensitivity", col = "dodgerblue")
legend(0.6,0.6,
       c('logistic', 'logistic splines', 'logistic binned', 'decision tree'),
       col=c("darkorange", "darkolivegreen", "plum4", "dodgerblue"),lwd=3)
```


# References

::: {#refs}
:::
